{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n",
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n",
      "... --> y\n",
      "..y --> u\n",
      ".yu --> h\n",
      "yuh --> e\n",
      "uhe --> n\n",
      "hen --> g\n",
      "eng --> .\n",
      "... --> d\n",
      "..d --> i\n",
      ".di --> o\n",
      "dio --> n\n",
      "ion --> d\n",
      "ond --> r\n",
      "ndr --> e\n",
      "dre --> .\n",
      "... --> x\n",
      "..x --> a\n",
      ".xa --> v\n",
      "xav --> i\n",
      "avi --> e\n"
     ]
    }
   ],
   "source": [
    "# STARTER CODE\n",
    "\n",
    "words = open('../l5_backprop_ninja/names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "for x,y in zip(Xtr[:20], Ytr[:20]):\n",
    "  print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])\n",
    "\n",
    "# Near copy paste of the layers we have developed in Part 3\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "C = torch.randn((vocab_size, n_embd))\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2966\n",
      "  10000/ 200000: 2.2322\n",
      "  20000/ 200000: 2.4111\n",
      "  30000/ 200000: 2.1004\n",
      "  40000/ 200000: 2.3157\n",
      "  50000/ 200000: 2.2104\n",
      "  60000/ 200000: 1.9653\n",
      "  70000/ 200000: 1.9767\n",
      "  80000/ 200000: 2.6738\n",
      "  90000/ 200000: 2.0837\n",
      " 100000/ 200000: 2.2730\n",
      " 110000/ 200000: 1.7491\n",
      " 120000/ 200000: 2.2891\n",
      " 130000/ 200000: 2.3443\n",
      " 140000/ 200000: 2.1731\n",
      " 150000/ 200000: 1.8246\n",
      " 160000/ 200000: 1.7614\n",
      " 170000/ 200000: 2.2419\n",
      " 180000/ 200000: 2.0803\n",
      " 190000/ 200000: 2.1326\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb]\n",
    "  x = emb.view(emb.shape[0], -1)\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x132c2c190>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)\n",
    "\n",
    "# STARTER CODE END"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note: our batch size is very small right now (32 elements). This means that the variance in our losses will be high. In order to get a better representation of the loss, we could consider using a larger batch-size (resulting in a smoother curve), or represent loss as an average to filter out a lot of the noise we see in the plot above. We will address that shortly.\n",
    "\n",
    "\n",
    "First, we can analyze how the starter code performs once the model is trained. We will need to make sure to set our batchnorm layers to eval mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0583250522613525\n",
      "val 2.1065289974212646\n"
     ]
    }
   ],
   "source": [
    "for layer in layers:\n",
    "    layer.training=False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "\n",
    "    emb = C[x]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate some sample names from the model -- at this point our names are starting to look somewhat reasonable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivon.\n",
      "fanili.\n",
      "thoommara.\n",
      "kelo.\n",
      "matyn.\n",
      "leandr.\n",
      "aleigh.\n",
      "koldeniah.\n",
      "prus.\n",
      "carleen.\n",
      "jah.\n",
      "jorra.\n",
      "alaya.\n",
      "shonan.\n",
      "vishylaharia.\n",
      "juna.\n",
      "vio.\n",
      "orven.\n",
      "mina.\n",
      "laylee.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "         # forward pass the neural net\n",
    "        emb = C[torch.tensor([context])]\n",
    "      \n",
    "        x = emb.view(emb.shape[0], -1)\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        logits = x\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the loss plot, one very simple fix is to change the shape of our loss tensor such that we organize it into uniform-sized rows. Then, we can take the mean of each row of consecutive elements to get a smoother curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x132b3bc70>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_size = 1000\n",
    "\n",
    "plt.plot(torch.tensor(lossi).view(-1,chunk_size).mean(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we can continue to organize our forward progation code into modules, right now we have to write 5 lines of code to propagate our data through the network and calculate loss. If we do a better job of organizing we can make that much nicer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can create modules for the embedding and flatten operations we perform via:\n",
    "```\n",
    "emb = C[x]\n",
    "x = emb.view(emb.shape[0], -1)\n",
    "```\n",
    "We can achieve this by just compartmentalizing those operations into simple modules -- this way they become more composable with the modules we've already defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim) -> None:\n",
    "        self.weight = torch.randn((num_embeddings, embedding_dim))\n",
    "    def __call__(self, IX):\n",
    "        self.out = self.weight[IX]\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "    \n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can modify our network initialization and forward pass code to use these new modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n",
      "      0/ 200000: 3.2966\n",
      "  10000/ 200000: 2.2322\n",
      "  20000/ 200000: 2.4111\n",
      "  30000/ 200000: 2.1004\n",
      "  40000/ 200000: 2.3157\n",
      "  50000/ 200000: 2.2104\n",
      "  60000/ 200000: 1.9653\n",
      "  70000/ 200000: 1.9767\n",
      "  80000/ 200000: 2.6738\n",
      "  90000/ 200000: 2.0837\n",
      " 100000/ 200000: 2.2730\n",
      " 110000/ 200000: 1.7491\n",
      " 120000/ 200000: 2.2891\n",
      " 130000/ 200000: 2.3443\n",
      " 140000/ 200000: 2.1731\n",
      " 150000/ 200000: 1.8246\n",
      " 160000/ 200000: 1.7614\n",
      " 170000/ 200000: 2.2419\n",
      " 180000/ 200000: 2.0803\n",
      " 190000/ 200000: 2.1326\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "\n",
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "layers = [\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  Flatten(),\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "parameters = [p for layer in layers for p in layer.parameters()] # now we don't need to handle embedding parameters with a special case \n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# forward pass\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  x = Xb\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we can make our implementation more elegant is using a *container*, which is a notion pytorch uses to describe a module that is composed of other modules. We can write a simple 'Sequential' module, which threads its input through each of its contained layers in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can update our network init and forward pass once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n",
      "      0/ 200000: 3.2966\n",
      "  10000/ 200000: 2.2322\n",
      "  20000/ 200000: 2.4111\n",
      "  30000/ 200000: 2.1004\n",
      "  40000/ 200000: 2.3157\n",
      "  50000/ 200000: 2.2104\n",
      "  60000/ 200000: 1.9653\n",
      "  70000/ 200000: 1.9767\n",
      "  80000/ 200000: 2.6738\n",
      "  90000/ 200000: 2.0837\n",
      " 100000/ 200000: 2.2730\n",
      " 110000/ 200000: 1.7491\n",
      " 120000/ 200000: 2.2891\n",
      " 130000/ 200000: 2.3443\n",
      " 140000/ 200000: 2.1731\n",
      " 150000/ 200000: 1.8246\n",
      " 160000/ 200000: 1.7614\n",
      " 170000/ 200000: 2.2419\n",
      " 180000/ 200000: 2.0803\n",
      " 190000/ 200000: 2.1326\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "\n",
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  Flatten(),\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True\n",
    "\n",
    "# forward pass\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  x = model(Xb)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in model.parameters():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in model.parameters():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to update our evaluation and sampling code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0583250522613525\n",
      "val 2.1065289974212646\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training=False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    x = model(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miah.\n",
      "vishylaharia.\n",
      "juna.\n",
      "vio.\n",
      "orven.\n",
      "mina.\n",
      "laylee.\n",
      "esreezeal.\n",
      "wyla.\n",
      "bord.\n",
      "ser.\n",
      "apgbe.\n",
      "saan.\n",
      "krison.\n",
      "jevellrah.\n",
      "rid.\n",
      "jayden.\n",
      "layx.\n",
      "syrias.\n",
      "cir.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "         # forward pass the neural net\n",
    "      \n",
    "        x = model(torch.tensor([context]))\n",
    "        probs = F.softmax(x, dim=1)\n",
    "        # sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        # shift the context window and track the samples\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        # if we sample the special '.' token, break\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the model, one thing we could try is using a larger context size (block_size) when predicting the next character. We can try increasing the block size from 3 up to 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 8]) torch.Size([182625])\n",
      "torch.Size([22655, 8]) torch.Size([22655])\n",
      "torch.Size([22866, 8]) torch.Size([22866])\n",
      "22097\n",
      "      0/ 200000: 3.2847\n",
      "  10000/ 200000: 2.0647\n",
      "  20000/ 200000: 1.9722\n",
      "  30000/ 200000: 2.0948\n",
      "  40000/ 200000: 1.9738\n",
      "  50000/ 200000: 2.1287\n",
      "  60000/ 200000: 2.3574\n",
      "  70000/ 200000: 1.9131\n",
      "  80000/ 200000: 2.0735\n",
      "  90000/ 200000: 2.0968\n",
      " 100000/ 200000: 1.4963\n",
      " 110000/ 200000: 2.1294\n",
      " 120000/ 200000: 2.2324\n",
      " 130000/ 200000: 2.2071\n",
      " 140000/ 200000: 2.2326\n",
      " 150000/ 200000: 1.8908\n",
      " 160000/ 200000: 1.6867\n",
      " 170000/ 200000: 2.0968\n",
      " 180000/ 200000: 1.7824\n",
      " 190000/ 200000: 1.9151\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 8 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n",
    "\n",
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  Flatten(),\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True\n",
    "\n",
    "# forward pass\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  x = model(Xb)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in model.parameters():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in model.parameters():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.9163438081741333\n",
      "val 2.034247636795044\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training=False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    x = model(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we see some improvement with this larger block size, we still are ultimately combinining the context together in a single step, which might be something that would be better to do more incrementally.\n",
    "\n",
    "\n",
    "Instead, we can implement WaveNet, which uses a heirarchical structure to combine context elements over the course of many layers, while maintaining some notion of locality amongst context inputs. https://arxiv.org/abs/1609.03499"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement this structure, we need to join pairs of elements in each layer. In our MLP example, we join every input element in the first layer, via flattening the encodings and feeding the result into a linear layer. Instead, we need to create groups for each pair, such that the neurons that each group feed into are isolated from other groups."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, instead of our flatten layer resulting in a tensor of the shape (X, 80)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([22655, 8, 10]), torch.Size([22655, 80]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].out.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want (X, 4, 20) for 4 groups of two from a block of 8 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22655, 4, 20])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].out.view(-1, 4, 20).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement a new Flatten layer that gives the desired behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenConsecutive:\n",
    "    # flattens n consecutive elements\n",
    "    def __init__(self, n) -> None:\n",
    "        self.n = n\n",
    "    \n",
    "    def __call__(self, x):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        # we still want the B samples in the batch, but instead of T embeddings of size C concatenated toghether, we want to concatenate each of n groups of consecutive embeddings\n",
    "        x = x.view(B, T//self.n, C*self.n) \n",
    "        # if this results in only 1 group, that dimension is unnessecary\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1) # remove the extra dimension\n",
    "\n",
    "        self.out = x\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can test it with n == block_size, this should yield the same behavior as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  FlattenConsecutive(block_size),\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run a small example through the network to see that our output shapes after each layer are as we expect them to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (4, 8, 10)\n",
      "FlattenConsecutive : (4, 80)\n",
      "Linear : (4, 200)\n",
      "BatchNorm1d : (4, 200)\n",
      "Tanh : (4, 200)\n",
      "Linear : (4, 27)\n"
     ]
    }
   ],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's update the group size to be smaller than the block size. We can reduce the number of groups until we have a single group remaining, so 4 groups, then 2, then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170897\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "group_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_embd * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_hidden * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_hidden * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our implementation yields the desired group sizes as we propagate through the layers, we divide out 8 embeddings into groups of two until we have a single group -- acheiving the heirarchical WaveNet architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (4, 8, 10)\n",
      "FlattenConsecutive : (4, 4, 20)\n",
      "Linear : (4, 4, 200)\n",
      "BatchNorm1d : (4, 4, 200)\n",
      "Tanh : (4, 4, 200)\n",
      "FlattenConsecutive : (4, 2, 400)\n",
      "Linear : (4, 2, 200)\n",
      "BatchNorm1d : (4, 2, 200)\n",
      "Tanh : (4, 2, 200)\n",
      "FlattenConsecutive : (4, 400)\n",
      "Linear : (4, 200)\n",
      "BatchNorm1d : (4, 200)\n",
      "Tanh : (4, 200)\n",
      "Linear : (4, 27)\n"
     ]
    }
   ],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One adjustment we can make is scaling down the number of parameters... we can preserve the heirarchical structure while reducing the number of parameters to be closer to the number we had with the MLP implementation... at 170k we've almost 8x-ed the number of parameters in our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22397\n",
      "Embedding : (4, 8, 10)\n",
      "FlattenConsecutive : (4, 4, 20)\n",
      "Linear : (4, 4, 68)\n",
      "BatchNorm1d : (4, 4, 68)\n",
      "Tanh : (4, 4, 68)\n",
      "FlattenConsecutive : (4, 2, 136)\n",
      "Linear : (4, 2, 68)\n",
      "BatchNorm1d : (4, 2, 68)\n",
      "Tanh : (4, 2, 68)\n",
      "FlattenConsecutive : (4, 136)\n",
      "Linear : (4, 68)\n",
      "BatchNorm1d : (4, 68)\n",
      "Tanh : (4, 68)\n",
      "Linear : (4, 27)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 68 # the number of neurons in the hidden layer of the MLP\n",
    "group_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_embd * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_hidden * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_hidden * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a hidden layer size of 68 we have exactly the number of parameters we had before! Next we can test that this implementation is still effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3068\n",
      "  10000/ 200000: 2.1772\n",
      "  20000/ 200000: 2.4289\n",
      "  30000/ 200000: 2.2455\n",
      "  40000/ 200000: 2.0728\n",
      "  50000/ 200000: 2.1918\n",
      "  60000/ 200000: 2.1698\n",
      "  70000/ 200000: 1.8329\n",
      "  80000/ 200000: 1.9106\n",
      "  90000/ 200000: 1.9519\n",
      " 100000/ 200000: 1.7333\n",
      " 110000/ 200000: 2.4024\n",
      " 120000/ 200000: 2.2626\n",
      " 130000/ 200000: 2.0729\n",
      " 140000/ 200000: 2.2658\n",
      " 150000/ 200000: 1.9702\n",
      " 160000/ 200000: 2.1914\n",
      " 170000/ 200000: 2.1956\n",
      " 180000/ 200000: 2.0618\n",
      " 190000/ 200000: 1.9979\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  x = model(Xb)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in model.parameters():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in model.parameters():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1.9448806047439575\n",
      "val 2.0309791564941406\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    layer.training=False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte)\n",
    "    }[split]\n",
    "    x = model(x)\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is nearly identical to our MLP's performance -- but we have an issue that needs to be addressed: we have not updated our BatchNorm layer to handle its new input shape."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the statistics tracked by the first BatchNorm layer, we can see that they take on the shape (1,4,num_channels) whereas before they took on the shape (1,num_channels).\n",
    "So instead of computing the mean/var for each channel, we instead are computing the mean/variance for each group in each channel! However, we'd rather maintain statstics over the entire batch rather than subsections of the batch. It is not useful to compute means/var for each group rather than across the entire batch -- and computing across a larger set of samples means that our statistics will be more stable/accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 68]), torch.Size([1, 4, 68]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].running_mean.shape, model.layers[3].running_var.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our BatchNorm layer to fix this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      # compute statistics over every dimensions except for channels\n",
    "      if x.ndim == 2:\n",
    "        dim = (0,)\n",
    "      elif x.ndim == 3: \n",
    "        dim = (0, 1)\n",
    "\n",
    "      xmean = x.mean(dim, keepdim=True) # batch mean\n",
    "      xvar = x.var(dim, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22397\n",
      "Embedding : (4, 8, 10)\n",
      "FlattenConsecutive : (4, 4, 20)\n",
      "Linear : (4, 4, 68)\n",
      "BatchNorm1d : (4, 4, 68)\n",
      "Tanh : (4, 4, 68)\n",
      "FlattenConsecutive : (4, 2, 136)\n",
      "Linear : (4, 2, 68)\n",
      "BatchNorm1d : (4, 2, 68)\n",
      "Tanh : (4, 2, 68)\n",
      "FlattenConsecutive : (4, 136)\n",
      "Linear : (4, 68)\n",
      "BatchNorm1d : (4, 68)\n",
      "Tanh : (4, 68)\n",
      "Linear : (4, 27)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42); # seed rng for reproducibility\n",
    "# original network\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 68 # the number of neurons in the hidden layer of the MLP\n",
    "group_size = 2\n",
    "\n",
    "model = Sequential([\n",
    "  Embedding(vocab_size, n_embd), # add the new modules!\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_embd * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_hidden * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  FlattenConsecutive(group_size),\n",
    "  Linear(n_hidden * group_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size)\n",
    "])\n",
    "\n",
    "\n",
    "# parameter init\n",
    "with torch.no_grad():\n",
    "  model.layers[-1].weight *= 0.1 # last layer make less confident\n",
    "\n",
    "\n",
    "print(sum(p.nelement() for p in model.parameters())) # number of parameters in total\n",
    "for p in model.parameters():\n",
    "  p.requires_grad = True\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xb)\n",
    "\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, ':', tuple(layer.out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 68]), torch.Size([1, 1, 68]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].running_mean.shape, model.layers[3].running_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3109\n",
      "  10000/ 200000: 2.1318\n",
      "  20000/ 200000: 2.3410\n",
      "  30000/ 200000: 2.2023\n",
      "  40000/ 200000: 2.0870\n",
      "  50000/ 200000: 2.2679\n",
      "  60000/ 200000: 2.0068\n",
      "  70000/ 200000: 1.9561\n",
      "  80000/ 200000: 1.9424\n",
      "  90000/ 200000: 1.7969\n",
      " 100000/ 200000: 1.6260\n",
      " 110000/ 200000: 2.3590\n",
      " 120000/ 200000: 2.4077\n",
      " 130000/ 200000: 2.0557\n",
      " 140000/ 200000: 2.2412\n",
      " 150000/ 200000: 1.9585\n",
      " 160000/ 200000: 2.0717\n",
      " 170000/ 200000: 2.2240\n",
      " 180000/ 200000: 1.9862\n",
      " 190000/ 200000: 1.9365\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  x = model(Xb)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in model.parameters():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update: simple SGD\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in model.parameters():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the words dataset and create char to id and id to char mappings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../l2_makemore/names.txt').read().splitlines()\n",
    "chars = ['.'] + sorted(list(set(''.join(words))))\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we will consider the 3 previous characters to predict the next character in a name. As we did in the previous lecture, we can use the '.' character to denote start and end of sequence.\n",
    "\n",
    "We can print a few examples to see that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... -----> e\n",
      "..e -----> m\n",
      ".em -----> m\n",
      "emm -----> a\n",
      "mma -----> .\n",
      "... -----> o\n",
      "..o -----> l\n",
      ".ol -----> i\n",
      "oli -----> v\n",
      "liv -----> i\n",
      "ivi -----> a\n",
      "via -----> .\n",
      "... -----> a\n",
      "..a -----> v\n",
      ".av -----> a\n",
      "ava -----> .\n",
      "... -----> i\n",
      "..i -----> s\n",
      ".is -----> a\n",
      "isa -----> b\n",
      "sab -----> e\n",
      "abe -----> l\n",
      "bel -----> l\n",
      "ell -----> a\n",
      "lla -----> .\n",
      "... -----> s\n",
      "..s -----> o\n",
      ".so -----> p\n",
      "sop -----> h\n",
      "oph -----> i\n",
      "phi -----> a\n",
      "hia -----> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    context = [0] * block_size\n",
    "    for c in w + '.':\n",
    "        ix = stoi[c]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '----->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create the embedding vectors. We can first experiment with a smaller size -- I chose 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 3), requires_grad=True)\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layer will connect the embeddings to a linear layer, we choose a size of 100 neurons. \n",
    "\n",
    "The input size will be the block_size (3) multiplied by the embedding size (also 3) = 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((9, 100), requires_grad=True)\n",
    "b1 = torch.randn(100, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the embeddings into the hidden layer, we will need to squash the dimensions such that they are of the shape (X, 9) instead of (X, 3, 3) which we can accomplish with the *view* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2684,  0.8108, -1.9514,  ..., -6.0133,  4.7472, -6.3218],\n",
       "        [ 0.8325, -0.0911, -2.8809,  ..., -6.3895,  2.3085, -5.3702],\n",
       "        [-0.6168,  5.4683, -1.9259,  ..., -4.0562,  2.4511, -3.2630],\n",
       "        ...,\n",
       "        [-4.1155,  1.6355, -4.7648,  ..., -2.9742, -2.8320, -3.8669],\n",
       "        [-2.2059,  2.6049, -2.5793,  ...,  2.9790,  0.6621,  1.7337],\n",
       "        [-4.2592,  1.9306, -1.2945,  ...,  6.2807, -0.7707,  4.9068]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(emb.view(-1, 9) @ W1) + b1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output layer will map the 100 activations from the hidden layer to 27 activations that we will use to represent the probability distribution for each of the 27 possible characters.\n",
    "\n",
    "In total, the parameters in our network include:\n",
    " * embedding weights (C)\n",
    " * hidden layer weights (W1)\n",
    " * hidden layer biases (b1)\n",
    " * output layer weights (W2)\n",
    " * output layer biases (b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a forward propogation function to propogate data through the network. Note that we do not apply an activation to the output layer yet (because we are going to use Cross Entropy Loss! which assumes the values are still logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, parameters):\n",
    "    C, W1, b1, W2, b2 = parameters\n",
    "    emb = C[x]\n",
    "    h = (emb.view(-1, emb.shape[1] * emb.shape[2]) @ W1) + b1\n",
    "    h = h.tanh()\n",
    "    h = (h @ W2) + b2\n",
    "    return h # returns logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(X, parameters).shape, X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can write a function the performs one step of optimization. It feeds some input through the network, computes the cross entropy loss on the output logits, then performs a gradient descent step on all the paramter weights and resets the gradients to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_step(x, y, parameters, lr=0.1):\n",
    "    logits = forward(x, parameters)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * 0.1\n",
    "        p.grad = None\n",
    "    return loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that running 2 optimization steps decreases the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.689239501953125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_step(X, Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.849098205566406"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_step(X, Y, parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function would be a way to instantiate a network with different layer hyperparamters so that we can easily experiment with different configurations. We can wrap the instantiation of each of the layers in *init_parameters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(num_chars, block_size, embedding_size, hidden_layer_size):\n",
    "    C = torch.randn((num_chars, embedding_size), requires_grad=True)\n",
    "    W1 = torch.randn((embedding_size * block_size, hidden_layer_size), requires_grad=True)\n",
    "    b1 = torch.randn(hidden_layer_size, requires_grad=True)\n",
    "    W2 = torch.randn((hidden_layer_size, num_chars), requires_grad=True)\n",
    "    b2 = torch.randn(num_chars, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = init_parameters(27, 3, 3, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to mess around with changing the block size, so we can create a function that creates input data with a given block size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs_and_labels(words, block_size):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for c in w + '.':\n",
    "            ix = stoi[c]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, now that we are using a much larger number of paramters, we need to make sure that we don't accidentally overfit to the data we train on. To combat this, we will split the data into a training set, a validation set, and a test set wtih this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_split(words, train_size=0.8, dev_size=0.1):\n",
    "    random.shuffle(words)\n",
    "    assert train_size + dev_size < 1\n",
    "    num_words = len(words)\n",
    "    train_split = int(num_words * train_size)\n",
    "    dev_split = int(num_words * (train_size + dev_size))\n",
    "    train_words = words[:train_split]\n",
    "    dev_words = words[train_split:dev_split]\n",
    "    test_words = words[dev_split:]\n",
    "    return train_words, dev_words, test_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the data and create input tensors for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, dev_words, test_words = train_dev_test_split(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_inputs_and_labels(train_words, block_size)\n",
    "X_dev, Y_dev = build_inputs_and_labels(dev_words, block_size)\n",
    "X_test, Y_test = build_inputs_and_labels(test_words, block_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we can write a gradient descent algorithm. The features include:\n",
    "* Minibatches: It's very slow to perform a gradient descent step on the entire dataset. Instead, we can samples a subset of the data and perform a gradient descent step on the sample, which should allow us to converge more quickly.\n",
    "* lr_deacy: Modern optimizers use all sorts of techniques to fine-tune the learning rate during training. For our purposes, we can just apply a decay factor over time s.t. (new lr) = (1 - decay_rate) * old_lr\n",
    "* Regularization: We can also include a regularization term in the loss function, which will normalize the model's output (by punishing large weights) and help prevent against overfitting. My implementation is very janky but does the job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, parameters, iterations=100, minibatch_size=100, lr=0.1, lr_decay=0.001, regularization=0.01, print_every=10):\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        batch_indices = torch.randint(0, x.shape[0], (minibatch_size,))\n",
    "        xi = x[batch_indices]\n",
    "        yi = y[batch_indices]\n",
    "        reg_term = torch.sum(torch.stack([(p**2).sum() for p in parameters])).divide(torch.sum(torch.tensor([p.numel() for p in parameters]))) * regularization\n",
    "        loss = optimize_step(xi, yi, parameters, lr) + reg_term\n",
    "        losses.append(loss)\n",
    "        if i % print_every == 0:\n",
    "            print(f'Iteration {i} loss: {loss}')\n",
    "        lr = lr * (1 - lr_decay)\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the topic of learning rate decay, we can play around with a learning rate and a decay rate to see how the learning rate changes over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12accf490>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAA9hAAAPYQGoP6dpAABP6UlEQVR4nO3dfVxUZf4//teZGWYAYYZ7BhQB7+/BUEbM1FY2tHbLsl01W81Mq1U3Y7uRvqa1228xtdYsy912S3fLNPuUlRWb4k03jncomnekpKLCcCsz3MgMzJzfHyNjkygMAmeYeT0fj/NgOOc6Z95zJc3rcc51nSOIoiiCiIiIyIPIpC6AiIiIqK0x4BAREZHHYcAhIiIij8OAQ0RERB6HAYeIiIg8DgMOEREReRwGHCIiIvI4DDhERETkcRRSF9BRbDYbCgsLERgYCEEQpC6HiIiIWkAURVRVVSE6OhoyWcvPy3hNwCksLERMTIzUZRAREVErnD9/Ht26dWtxe68JOIGBgQDsHaRWqyWuhoiIiFrCZDIhJibG8T3eUl4TcBovS6nVagYcIiKiTsbV4SUcZExEREQehwGHiIiIPA4DDhEREXkcBhwiIiLyOAw4RERE5HEYcIiIiMjjMOAQERGRx2HAISIiIo/DgENEREQep1UBZ/Xq1YiLi4Ovry90Oh327dt33bZvv/02brvtNgQHByM4OBipqanXtBdFEYsXL0ZUVBT8/PyQmpqKU6dOObWpqKjAtGnToFarERQUhFmzZqG6uro15RMREZGHczngbNy4Eenp6ViyZAkOHjyIhIQEpKWloaSkpMn2O3fuxNSpU7Fjxw7o9XrExMTgjjvuwMWLFx1tli1bhlWrVmHNmjXYu3cvunTpgrS0NNTV1TnaTJs2DceOHcPWrVuxZcsWfPPNN5gzZ04rPjIRERF5PNFFycnJ4ty5cx2/W61WMTo6WszMzGzR/g0NDWJgYKC4bt06URRF0WaziVqtVly+fLmjTWVlpahSqcQPPvhAFEVRPH78uAhA3L9/v6PNV199JQqCIF68eLFF72s0GkUAotFobFF7IiIikl5rv79dOoNjsViQk5OD1NRUxzqZTIbU1FTo9foWHaO2thb19fUICQkBAJw5cwYGg8HpmBqNBjqdznFMvV6PoKAgDBs2zNEmNTUVMpkMe/fubfJ9zGYzTCaT09IeThVXYelXJ7FmV367HJ+IiIhc51LAKSsrg9VqRWRkpNP6yMhIGAyGFh3j2WefRXR0tCPQNO53o2MaDAZEREQ4bVcoFAgJCbnu+2ZmZkKj0TiWmJiYFtXnqnPltVizKx+bDpxvl+MTERGR6zp0FtXSpUuxYcMGfPLJJ/D19W3X98rIyIDRaHQs58+3TwBJig0GAOSX1uBSjaVd3oOIiIhc41LACQsLg1wuR3FxsdP64uJiaLXaG+67YsUKLF26FF9//TWGDBniWN+4342OqdVqrxnE3NDQgIqKiuu+r0qlglqtdlraQ3AXJXqGdwEA5Jy71C7vQURERK5xKeAolUokJSUhOzvbsc5msyE7OxspKSnX3W/ZsmX461//iqysLKdxNAAQHx8PrVbrdEyTyYS9e/c6jpmSkoLKykrk5OQ42mzfvh02mw06nc6Vj9AuhsXaxxMdYMAhIiJyCy5fokpPT8fbb7+NdevW4cSJE3j88cdRU1ODmTNnAgCmT5+OjIwMR/uXX34Zzz//PN555x3ExcXBYDDAYDA47mEjCAIWLFiAl156CZ999hl++OEHTJ8+HdHR0Zg4cSIAoH///hg/fjxmz56Nffv24fvvv8e8efMwZcoUREdHt0E33JzGy1QHGXCIiIjcgsLVHSZPnozS0lIsXrwYBoMBiYmJyMrKcgwSLigogEx2NTe99dZbsFgsuP/++52Os2TJErzwwgsAgGeeeQY1NTWYM2cOKisrMWrUKGRlZTmN03n//fcxb948jBs3DjKZDJMmTcKqVata85nbXFKcPeAcvlAJS4MNSgVvEE1ERCQlQRRFUeoiOoLJZIJGo4HRaGzz8TiiKOKWv27Fpdp6fPzHkbile3CbHp+IiMhbtfb7m6ca2oAgCLxMRURE5EYYcNpIUuNA47MMOERERFJjwGkjw66Mwzlw7hK85KofERGR22LAaSODu2rgIxdQVm1GQUWt1OUQERF5NQacNuLrI8egrhoAvOEfERGR1Bhw2tCw2KuXqYiIiEg6DDhtqHGgcQ4HGhMREUmKAacNNU4V/7GkCsbL9RJXQ0RE5L0YcNpQeKAKsaH+EEXgUAHP4hAREUmFAaeNNZ7F4UBjIiIi6TDgtLFhvOEfERGR5Bhw2ljjGZzc85Wot9okroaIiMg7MeC0sd4RAdD4+eByvRXHCk1Sl0NEROSVGHDamEwmYHic/TLVvjPlEldDRETknRhw2sGIHo0Bp0LiSoiIiLwTA047SI6/GnBsNj54k4iIqKMx4LSDAVFqdFHKYaprQF5xldTlEBEReR0GnHagkMuQFMfLVERERFJhwGknuiuXqfZyoDEREVGHY8BpJz8fhyOKHIdDRETUkRhw2smQbhooFTKUVVvwU1mN1OUQERF5FQacdqJSyDE0JggAx+EQERF1NAacdqSL50BjIiIiKTDgtKPk+FAADDhEREQdjQGnHd0SGwSFTMDFysu4cKlW6nKIiIi8BgNOO/JXKjCoqwYAz+IQERF1JAacdqbjc6mIiIg6HANOO+NAYyIioo7HgNPOkmJDIAjAT2U1KDHVSV0OERGRV2DAaWcaPx8MiFIDAPbwLA4REVGHYMDpACk97NPF9fllEldCRETkHVoVcFavXo24uDj4+vpCp9Nh375912177NgxTJo0CXFxcRAEAStXrrymTeO2Xy5z5851tBk7duw12x977LHWlN/hRvayB5zd+XzwJhERUUdwOeBs3LgR6enpWLJkCQ4ePIiEhASkpaWhpKSkyfa1tbXo0aMHli5dCq1W22Sb/fv3o6ioyLFs3boVAPC73/3Oqd3s2bOd2i1btszV8iUxPC4EcpmAc+W1uFh5WepyiIiIPJ7LAefVV1/F7NmzMXPmTAwYMABr1qyBv78/3nnnnSbbDx8+HMuXL8eUKVOgUqmabBMeHg6tVutYtmzZgp49e2LMmDFO7fz9/Z3aqdVqV8uXRKCvDwZfuR+OnmdxiIiI2p1LAcdisSAnJwepqalXDyCTITU1FXq9vk0KslgseO+99/Dwww9DEASnbe+//z7CwsIwaNAgZGRkoLa289wdeGTPxnE4DDhERETtTeFK47KyMlitVkRGRjqtj4yMxMmTJ9ukoM2bN6OyshIPPfSQ0/oHHngAsbGxiI6OxpEjR/Dss88iLy8PH3/8cZPHMZvNMJvNjt9NJlOb1NdaI3uG4c2d+dDnl0EUxWvCGxEREbUdlwJOR/j3v/+NCRMmIDo62mn9nDlzHK8HDx6MqKgojBs3Dvn5+ejZs+c1x8nMzMSLL77Y7vW2VFJsMHzkAgqNdThXXou4sC5Sl0REROSxXLpEFRYWBrlcjuLiYqf1xcXF1x1A7Ipz585h27ZteOSRR5ptq9PpAACnT59ucntGRgaMRqNjOX/+/E3XdzP8lHIM7R4MAND/xMtURERE7cmlgKNUKpGUlITs7GzHOpvNhuzsbKSkpNx0Me+++y4iIiJw1113Nds2NzcXABAVFdXkdpVKBbVa7bRIrXEcDqeLExERtS+XL1Glp6djxowZGDZsGJKTk7Fy5UrU1NRg5syZAIDp06eja9euyMzMBGAfNHz8+HHH64sXLyI3NxcBAQHo1auX47g2mw3vvvsuZsyYAYXCuaz8/HysX78ed955J0JDQ3HkyBE8+eSTGD16NIYMGdLqD9/RUnqEYiVOQZ9fznE4RERE7cjlgDN58mSUlpZi8eLFMBgMSExMRFZWlmPgcUFBAWSyqyeGCgsLMXToUMfvK1aswIoVKzBmzBjs3LnTsX7btm0oKCjAww8/fM17KpVKbNu2zRGmYmJiMGnSJCxatMjV8iWV2D0Ivj4ylFWbcbqkGr0jA6UuiYiIyCMJoiiKUhfREUwmEzQaDYxGo6SXqx781158d7oML949EDNGxklWBxERUWfQ2u9vPouqg6XwfjhERETtjgGngzlu+PdTOWw2rzh5RkRE1OEYcDrY4K4aBKgUMF6ux/EiaW8+SERE5KkYcDqYQi5DcnwIAGB3fpnE1RAREXkmBhwJjOoVBgD49hQDDhERUXtgwJHA6D72gLPvTAXq6q0SV0NEROR5GHAk0DM8AFq1L8wNNhw4e0nqcoiIiDwOA44EBEHAqN6Nl6lKJa6GiIjI8zDgSOS23hyHQ0RE1F4YcCRy65WBxseLTCitMktcDRERkWdhwJFIWIAKA6Ptt5zmdHEiIqK2xYAjoVG8TEVERNQuGHAkdFuvcAD2gcZe8sxTIiKiDsGAI6FhccFQKWQoNplxuqRa6nKIiIg8BgOOhHx95I7HNnzDy1RERERthgFHYqN72y9Tfcf74RAREbUZBhyJNQ403vNTBcwNfGwDERFRW2DAkVg/bSDCAlS4XG/FwXOVUpdDRETkERhwJCYIws/uaszLVERERG2BAccNND5dfGceAw4REVFbYMBxA6N7h0MQ7I9tKDHVSV0OERFRp8eA4wZCA1QY0i0IALDzR57FISIiulkMOG5ibB/7dPGdeSUSV0JERNT5MeC4ibF9Gx/bUIZ6q03iaoiIiDo3Bhw3MaRbEEK6KFFV14CD5y5JXQ4REVGnxoDjJuQyAaOvTBfnOBwiIqKbw4DjRsb2jQDA6eJEREQ3iwHHjYzuY58ufqLIBIOR08WJiIhaiwHHjYR0USLhynTxXT9yNhUREVFrMeC4mcbZVDtO8jIVERFRazHguJnGcTjfneZ0cSIiotZiwHEzQ7pqENpFiWpzAw6c5XRxIiKi1mhVwFm9ejXi4uLg6+sLnU6Hffv2XbftsWPHMGnSJMTFxUEQBKxcufKaNi+88AIEQXBa+vXr59Smrq4Oc+fORWhoKAICAjBp0iQUFxe3pny3JpMJGN14V2OOwyEiImoVlwPOxo0bkZ6ejiVLluDgwYNISEhAWloaSkqa/jKura1Fjx49sHTpUmi12used+DAgSgqKnIs3333ndP2J598Ep9//jk2bdqEXbt2obCwEPfdd5+r5XcKjeNwtp9gwCEiImoNlwPOq6++itmzZ2PmzJkYMGAA1qxZA39/f7zzzjtNth8+fDiWL1+OKVOmQKVSXfe4CoUCWq3WsYSFhTm2GY1G/Pvf/8arr76KX/3qV0hKSsK7776L3bt3Y8+ePa5+BLc3tk8E5DIBp0qqUVBeK3U5REREnY5LAcdisSAnJwepqalXDyCTITU1FXq9/qYKOXXqFKKjo9GjRw9MmzYNBQUFjm05OTmor693et9+/fqhe/fu131fs9kMk8nktHQWGn8fDI8LBgBsO+F5l+GIiIjam0sBp6ysDFarFZGRkU7rIyMjYTAYWl2ETqfD2rVrkZWVhbfeegtnzpzBbbfdhqqqKgCAwWCAUqlEUFBQi983MzMTGo3GscTExLS6Pimk9rf3MQMOERGR69xiFtWECRPwu9/9DkOGDEFaWhq+/PJLVFZW4sMPP2z1MTMyMmA0Gh3L+fPn27Di9tcYcPadqYDxcr3E1RAREXUuLgWcsLAwyOXya2YvFRcX33AAsauCgoLQp08fnD59GgCg1WphsVhQWVnZ4vdVqVRQq9VOS2cSF9YFvSIC0GATsYsP3yQiInKJSwFHqVQiKSkJ2dnZjnU2mw3Z2dlISUlps6Kqq6uRn5+PqKgoAEBSUhJ8fHyc3jcvLw8FBQVt+r7uxnGZ6jgvUxEREblC4eoO6enpmDFjBoYNG4bk5GSsXLkSNTU1mDlzJgBg+vTp6Nq1KzIzMwHYByYfP37c8frixYvIzc1FQEAAevXqBQB46qmn8Nvf/haxsbEoLCzEkiVLIJfLMXXqVACARqPBrFmzkJ6ejpCQEKjVasyfPx8pKSkYMWJEm3SEO0rtH4E1u/KxM68E9VYbfORucUWRiIjI7bkccCZPnozS0lIsXrwYBoMBiYmJyMrKcgw8LigogEx29Yu4sLAQQ4cOdfy+YsUKrFixAmPGjMHOnTsBABcuXMDUqVNRXl6O8PBwjBo1Cnv27EF4eLhjv7///e+QyWSYNGkSzGYz0tLS8Oabb7b2c3cKQ7sHI6SLEhU1Fuw/W4GRPcOa34mIiIggiKIoSl1ERzCZTNBoNDAajZ1qPM5Tmw7jo5wLmDUqHs//ZoDU5RAREXWo1n5/85qHm0vtb3/45rYTxfCSLEpERHTTGHDc3G29w6GUy3CuvBanS6qlLoeIiKhTYMBxc11UCozsFQoA2MZnUxEREbUIA04nMI53NSYiInIJA04n0DgO52DBJZRWmSWuhoiIyP0x4HQCURo/JMQEQRSBr4+3/plfRERE3oIBp5MYP9D+SIqsoww4REREzWHA6STSBtrH4ejzy2Gs5cM3iYiIboQBp5PoER6AvpGBaLCJyD7JwcZEREQ3woDTiaQN4mUqIiKilmDA6UQax+Hs+rEUtZYGiashIiJyXww4nUj/qEB0D/GHucGGXXmlUpdDRETkthhwOhFBEDC+8TLVMV6mIiIiuh4GnE4m7cplqu0nSmBusEpcDRERkXtiwOlkhsYEISJQhSpzA3bnl0tdDhERkVtiwOlkZDLBcRbnf5xNRURE1CQGnE6ocRzO18eLYbWJEldDRETkfhhwOqHk+BAE+fugosaCvT/xMhUREdEvMeB0Qj5yGdIG2M/ibPmhSOJqiIiI3A8DTif1m4QoAPa7GjdYbRJXQ0RE5F4YcDqplB6hCO2iREWNhbOpiIiIfoEBp5NSyGWOwcZbjhRKXA0REZF7YcDpxH4zJBqA/TKVpYGXqYiIiBox4HRiyfEhCA9UwVTXgO9O89lUREREjRhwOjG5TMBdg+2Djbcc5mwqIiKiRgw4ndxdQ+wB5+vjxair57OpiIiIAAacTi+pezC0al9UmxvwzY+8TEVERAQw4HR6MpngOIuz5QgvUxEREQEMOB7hN1cCzrYTxbhs4WUqIiIiBhwPkBgThG7Bfqi1WLH9ZInU5RAREUmOAccDCILguCfOp7kXJa6GiIhIeq0KOKtXr0ZcXBx8fX2h0+mwb9++67Y9duwYJk2ahLi4OAiCgJUrV17TJjMzE8OHD0dgYCAiIiIwceJE5OXlObUZO3YsBEFwWh577LHWlO+R7h3aFQCwI68ElbUWiashIiKSlssBZ+PGjUhPT8eSJUtw8OBBJCQkIC0tDSUlTV8aqa2tRY8ePbB06VJotdom2+zatQtz587Fnj17sHXrVtTX1+OOO+5ATU2NU7vZs2ejqKjIsSxbtszV8j1WX20g+kepUW8V8QWfME5ERF5O4eoOr776KmbPno2ZM2cCANasWYMvvvgC77zzDhYuXHhN++HDh2P48OEA0OR2AMjKynL6fe3atYiIiEBOTg5Gjx7tWO/v73/dkETAvUOjcaLIhM2HLmKaLlbqcoiIiCTj0hkci8WCnJwcpKamXj2ATIbU1FTo9fo2K8poNAIAQkJCnNa///77CAsLw6BBg5CRkYHa2trrHsNsNsNkMjktnu7uhK4QBGD/2Us4X3H9viEiIvJ0LgWcsrIyWK1WREZGOq2PjIyEwWBok4JsNhsWLFiAW2+9FYMGDXKsf+CBB/Dee+9hx44dyMjIwH//+188+OCD1z1OZmYmNBqNY4mJiWmT+tyZVuOLlB6hADjYmIiIvJvLl6ja29y5c3H06FF89913TuvnzJnjeD148GBERUVh3LhxyM/PR8+ePa85TkZGBtLT0x2/m0wmrwg5E4d2xe78cnxy6CLm3t4LgiBIXRIREVGHc+kMTlhYGORyOYqLi53WFxcXt8nYmHnz5mHLli3YsWMHunXrdsO2Op0OAHD69Okmt6tUKqjVaqfFG4wfpIVKIUN+aQ2OFXr+ZTkiIqKmuBRwlEolkpKSkJ2d7Vhns9mQnZ2NlJSUVhchiiLmzZuHTz75BNu3b0d8fHyz++Tm5gIAoqKiWv2+nkjt64PUAfZLiJ8c4mUqIiLyTi5PE09PT8fbb7+NdevW4cSJE3j88cdRU1PjmFU1ffp0ZGRkONpbLBbk5uYiNzcXFosFFy9eRG5urtOZl7lz5+K9997D+vXrERgYCIPBAIPBgMuXLwMA8vPz8de//hU5OTk4e/YsPvvsM0yfPh2jR4/GkCFDbrYPPM69ifZ74nx2uBANVpvE1RAREXU8QRRF0dWd3njjDSxfvhwGgwGJiYlYtWqV45LR2LFjERcXh7Vr1wIAzp492+QZmTFjxmDnzp32Iq4zTuTdd9/FQw89hPPnz+PBBx/E0aNHUVNTg5iYGNx7771YtGhRiy89mUwmaDQaGI1Gj79cZWmwQfe3bbhUW4//PJyM0X3CpS6JiIioVVr7/d2qgNMZeVPAAYBFm3/Ae3sKMDExGiunDJW6HCIiolZp7fc3n0Xloe67xT5IO+uYAaa6eomrISIi6lgMOB5qaEwQeoZ3QV29DV8c4aMbiIjIuzDgeChBEPC7Yfb7/mw6cF7iaoiIiDoWA44Hu29oV8hlAg4WVOJ0SbXU5RAREXUYBhwPFqH2xdgrM6g25fAsDhEReQ8GHA/3u2H2wcYfH7zIe+IQEZHXYMDxcL/qF4mQLkqUVpnxzalSqcshIiLqEAw4Hk6pkOGexGgAwKYDFySuhoiIqGMw4HiB3yXZZ1NtO1GMihqLxNUQERG1PwYcLzAgWo1BXdWot4r4NJcP4CQiIs/HgOMlGs/ifHjgArzk6RxEROTFGHC8xD2J0VAqZDhRZMLhC0apyyEiImpXDDheIshfibsGRwEA1u89J3E1RERE7YsBx4s8oOsOAPj8cBEfwElERB6NAceLDIsNRu+IAFyut2LzIQ42JiIiz8WA40UEQXCcxVm/t4CDjYmIyGMx4HiZ+4Z2g0ohw0lDFQ4WVEpdDhERUbtgwPEyGn8f/GaI/c7G6/cWSFwNERFR+2DA8UKNl6m2HCmEsZaDjYmIyPMw4HihW7oHoZ82EOYGGz4+xOdTERGR52HA8UIcbExERJ6OAcdLTRzaFX4+cpwqqca+MxVSl0NERNSmGHC8lNrXBxOH2gcb/0fPOxsTEZFnYcDxYtNT4gAAWccMKDJelrYYIiKiNsSA48X6R6mRHB8Cq03E+3s4ZZyIiDwHA46Xe2hkHADgg30FqKu3SlsMERFRG2HA8XJ3DIhElMYX5TUWfHGkSOpyiIiI2gQDjpdTyGV4cEQsAGCd/iynjBMRkUdgwCFMGR4DpUKGIxeMOHS+UupyiIiIbhoDDiE0QIXfXnk+1X92n5W2GCIiojbAgEMAgBkj7ZepvvihCCVVdRJXQ0REdHNaFXBWr16NuLg4+Pr6QqfTYd++fddte+zYMUyaNAlxcXEQBAErV65s1THr6uowd+5chIaGIiAgAJMmTUJxcXFryqcmDOkWhKHdg1Bv5ZRxIiLq/FwOOBs3bkR6ejqWLFmCgwcPIiEhAWlpaSgpKWmyfW1tLXr06IGlS5dCq9W2+phPPvkkPv/8c2zatAm7du1CYWEh7rvvPlfLpxt4+NZ4AMB/95zjlHEiIurUBNHFaTM6nQ7Dhw/HG2+8AQCw2WyIiYnB/PnzsXDhwhvuGxcXhwULFmDBggUuHdNoNCI8PBzr16/H/fffDwA4efIk+vfvD71ejxEjRjRbt8lkgkajgdFohFqtduUje40Gqw1jlu/ExcrL+Nu9gx0P5CQiIpJKa7+/XTqDY7FYkJOTg9TU1KsHkMmQmpoKvV7vyqFcOmZOTg7q6+ud2vTr1w/du3e/7vuazWaYTCanhW5MIZfh4VH2szj/+u4n2GycMk5ERJ2TSwGnrKwMVqsVkZGRTusjIyNhMBhaVUBLjmkwGKBUKhEUFNTi983MzIRGo3EsMTExrarP20weHoNAXwV+Kq3B9pNNX3YkIiJydx47iyojIwNGo9GxnD9/XuqSOoUAlQIPJNsvTb397U8SV0NERNQ6LgWcsLAwyOXya2YvFRcXX3cAcVscU6vVwmKxoLKyssXvq1KpoFarnRZqmYdujYNCJmDvmQocuVApdTlEREQucyngKJVKJCUlITs727HOZrMhOzsbKSkprSqgJcdMSkqCj4+PU5u8vDwUFBS0+n3p+qI0fvhtgv3Gf29/e0biaoiIiFyncHWH9PR0zJgxA8OGDUNycjJWrlyJmpoazJw5EwAwffp0dO3aFZmZmQDsg4iPHz/ueH3x4kXk5uYiICAAvXr1atExNRoNZs2ahfT0dISEhECtVmP+/PlISUlp0Qwqct0jt8Xjk0MX8eUPRVg4oR+6BvlJXRIREVGLuRxwJk+ejNLSUixevBgGgwGJiYnIyspyDBIuKCiATHb1xFBhYSGGDh3q+H3FihVYsWIFxowZg507d7bomADw97//HTKZDJMmTYLZbEZaWhrefPPN1n5uasbAaA1u7RWK70+X453vzuD53wyQuiQiIqIWc/k+OJ0V74Pjup15JXjo3f3wV8qxe+GvEOSvlLokIiLyMh1yHxzyLmP6hKN/lBq1FivW8iGcRETUiTDg0HUJgoC5t/cEALz7/VlUmxskroiIiKhlGHDohiYMikKPsC4wXq7H+r3npC6HiIioRRhw6IbkMgGPjbWfxXn72zN8CCcREXUKDDjUrImJXRGt8UVplRkf5VyQuhwiIqJmMeBQs5QKGeaM7gEAWLMrHw1Wm8QVERER3RgDDrXIlOTuCAtQ4sKly/j8SKHU5RAREd0QAw61iK+PHA+PigcAvLkjHzabV9w+iYiIOikGHGqxB0fEItBXgVMl1fjqqEHqcoiIiK6LAYdaTO3rg1lXzuKs3PYjrDyLQ0REbooBh1zy8Kh4qK+cxfnihyKpyyEiImoSAw65RO3rg9m32WdUvcazOERE5KYYcMhlD90ahyB/H+SX1uDzw5xRRURE7ocBh1wW+POzONmneF8cIiJyOww41CozRsYh2N8HZ8pq8Gkuz+IQEZF7YcChVglQKfDoGPszqlZt51kcIiJyLww41GrTU2IR2kWJc+W1+L+DfEYVERG5DwYcajV/pQKPX3nS+Mptp/ikcSIichsMOHRTHhwRi2iNL4qMdfiP/qzU5RAREQFgwKGb5Osjx4Jf9wEArN6RD+PleokrIiIiYsChNjDplm7oHREA4+V6/GNXvtTlEBERMeDQzZPLBDyd1hcA8M73Z1BsqpO4IiIi8nYMONQmfj0gEkmxwairt2HltlNSl0NERF6OAYfahCAIWDihHwDgwwPnkV9aLXFFRETkzRhwqM0MjwvBuH4RsNpErPhfntTlEBGRF2PAoTb1zPh+kAnAV0cN2H+2QupyiIjISzHgUJvqqw3E5OExAIC/fH4cNpsocUVEROSNGHCozaX/ui8CVAr8cNGITw5dlLocIiLyQgw41ObCA1WY96teAIBl/zuJWkuDxBUREZG3YcChdjHz1jjEhPih2GTGml0/SV0OERF5GQYcahcqhRzPTegPAPjnN/korLwscUVERORNWhVwVq9ejbi4OPj6+kKn02Hfvn03bL9p0yb069cPvr6+GDx4ML788kun7YIgNLksX77c0SYuLu6a7UuXLm1N+dRBxg/SIjk+BHX1NizLOil1OURE5EVcDjgbN25Eeno6lixZgoMHDyIhIQFpaWkoKSlpsv3u3bsxdepUzJo1C4cOHcLEiRMxceJEHD161NGmqKjIaXnnnXcgCAImTZrkdKy//OUvTu3mz5/vavnUgQRBwPN3DYAgAJtzC5Fz7pLUJRERkZcQRFF0aR6vTqfD8OHD8cYbbwAAbDYbYmJiMH/+fCxcuPCa9pMnT0ZNTQ22bNniWDdixAgkJiZizZo1Tb7HxIkTUVVVhezsbMe6uLg4LFiwAAsWLHClXAeTyQSNRgOj0Qi1Wt2qY1DrPPPRYXx44AIGRKnx+fxRkMsEqUsiIqJOorXf3y6dwbFYLMjJyUFqaurVA8hkSE1NhV6vb3IfvV7v1B4A0tLSrtu+uLgYX3zxBWbNmnXNtqVLlyI0NBRDhw7F8uXL0dBw/dk5ZrMZJpPJaSFpPDO+H9S+ChwvMuH9veekLoeIiLyASwGnrKwMVqsVkZGRTusjIyNhMBia3MdgMLjUft26dQgMDMR9993ntP5Pf/oTNmzYgB07duDRRx/F3/72NzzzzDPXrTUzMxMajcaxxMTEtOQjUjsIC1Dh6fH251Qt/18eyqrNEldERESezu1mUb3zzjuYNm0afH19ndanp6dj7NixGDJkCB577DG88soreP3112E2N/1lmZGRAaPR6FjOnz/fEeXTdTyQ3B2DuqpRVdeApV9xwDEREbUvlwJOWFgY5HI5iouLndYXFxdDq9U2uY9Wq21x+2+//RZ5eXl45JFHmq1Fp9OhoaEBZ8+ebXK7SqWCWq12Wkg6cpmAv94zCADwUc4FHOBzqoiIqB25FHCUSiWSkpKcBv/abDZkZ2cjJSWlyX1SUlKc2gPA1q1bm2z/73//G0lJSUhISGi2ltzcXMhkMkRERLjyEUhCQ7sHY8qV51Qt2nwUDVabxBUREZGnUri6Q3p6OmbMmIFhw4YhOTkZK1euRE1NDWbOnAkAmD59Orp27YrMzEwAwBNPPIExY8bglVdewV133YUNGzbgwIED+Oc//+l0XJPJhE2bNuGVV1655j31ej327t2L22+/HYGBgdDr9XjyySfx4IMPIjg4uDWfmyTyzPh+yDpmwElDFf6jP4eHR8VLXRIREXkglwPO5MmTUVpaisWLF8NgMCAxMRFZWVmOgcQFBQWQya6eGBo5ciTWr1+PRYsW4bnnnkPv3r2xefNmDBo0yOm4GzZsgCiKmDp16jXvqVKpsGHDBrzwwgswm82Ij4/Hk08+ifT0dFfLJ4mFdFHimbR+eO6TH/DK13lIG6RF1yA/qcsiIiIP4/J9cDor3gfHfdhsIn7/Dz0OnLuE2/uG452HhkMQeG8cIiK6VofcB4eoLchkAjLvGwylXIYdeaX4/EiR1CUREZGHYcAhSfSODMTc23sBAF787Bgu1VgkroiIiDwJAw5J5vGxPdE7IgDlNRb8f1+ekLocIiLyIAw4JBmlQoalk4ZAEOz3xvnuVJnUJRERkYdgwCFJJcUGY/qIWABAxidHUGu5/vPFiIiIWooBhyT39Ph+iNb44nzFZbzMxzgQEVEbYMAhyQWoFFg6aQgAYJ3+HL4/zUtVRER0cxhwyC2M7hOOabruAIBnPjqCqrp6iSsiIqLOjAGH3MZzd/ZHTIgfLlZexktbOKuKiIhajwGH3EYXlQIr7k+AIAAbD5zH9pPFze9ERETUBAYcciu6HqGYdav9AZzP/t8PvAEgERG1CgMOuZ2n0vqiZ3gXlFaZsejTo/CSx6UREVEbYsAht+PrI8erv0+EQibgiyNF+CjngtQlERFRJ8OAQ24pISYIT/66DwBgyWfH8FNptcQVERFRZ8KAQ27rsTE9MaJHCGotVjyxIReWBpvUJRERUSfBgENuSy4TsHLyUAT5++CHi0a88nWe1CUREVEnwYBDbk2r8cXLV+5y/I9vfsK3p0olroiIiDoDBhxye2kDtY67HKd/eBhl1WaJKyIiInfHgEOdwqK7BqB3RABKq8x4YsMhWG2cOk5ERNfHgEOdgp9Sjjen3QI/Hzm+P12O17b9KHVJRETkxhhwqNPoHRmIpZMGAwBWbT+NHXklEldERETuigGHOpV7ErviwRH28ThPbszFhUu1EldERETuiAGHOp3nfzMAQ7ppUFlbj7nrD8HcYJW6JCIicjMMONTpqBRyrH7gFmj8fHD4fCVe2nJC6pKIiMjNMOBQpxQT4o+/T04AAPx3zzls2FcgcUVEROROGHCo0/pVv0ikX3le1fOfHsWBsxUSV0RERO6CAYc6tXm398KEQVrUW0U89t5BFFZelrokIiJyAww41KnJZAJW/C4B/bSBKKs249H/5qCunoOOiYi8HQMOdXpdVAq8PX0YQroo8cNFI5756AhEkXc6JiLyZgw45BFiQvzx5rRboJAJ+OxwId7cmS91SUREJCEGHPIYI3qE4oW7BwIAlv8vD5/mXpS4IiIikkqrAs7q1asRFxcHX19f6HQ67Nu374btN23ahH79+sHX1xeDBw/Gl19+6bT9oYcegiAITsv48eOd2lRUVGDatGlQq9UICgrCrFmzUF1d3ZryyYM9OCIWj4yKBwA8vekI9p3hzCoiIm/kcsDZuHEj0tPTsWTJEhw8eBAJCQlIS0tDSUnTzwXavXs3pk6dilmzZuHQoUOYOHEiJk6ciKNHjzq1Gz9+PIqKihzLBx984LR92rRpOHbsGLZu3YotW7bgm2++wZw5c1wtn7zAc3f2x/iBWlisNsz57wHklzIIExF5G0F0cTSmTqfD8OHD8cYbbwAAbDYbYmJiMH/+fCxcuPCa9pMnT0ZNTQ22bNniWDdixAgkJiZizZo1AOxncCorK7F58+Ym3/PEiRMYMGAA9u/fj2HDhgEAsrKycOedd+LChQuIjo5utm6TyQSNRgOj0Qi1Wu3KR6ZO6LLFiqlv70Hu+Up0D/HHx38cibAAldRlERGRi1r7/e3SGRyLxYKcnBykpqZePYBMhtTUVOj1+ib30ev1Tu0BIC0t7Zr2O3fuREREBPr27YvHH38c5eXlTscICgpyhBsASE1NhUwmw969e5t8X7PZDJPJ5LSQ9/BTyvGvGcPQPcQfBRW1eGTdAVy2cPo4EZG3cCnglJWVwWq1IjIy0ml9ZGQkDAZDk/sYDIZm248fPx7/+c9/kJ2djZdffhm7du3ChAkTYLVaHceIiIhwOoZCoUBISMh13zczMxMajcaxxMTEuPJRyQOEBajw7szh0Pj5IPd8Jf74fg7qrTapyyIiog7gFrOopkyZgrvvvhuDBw/GxIkTsWXLFuzfvx87d+5s9TEzMjJgNBody/nz59uuYOo0eoYH4N8zhsHXR4YdeaV4atNh2Gy8Rw4RkadzKeCEhYVBLpejuLjYaX1xcTG0Wm2T+2i1WpfaA0CPHj0QFhaG06dPO47xy0HMDQ0NqKiouO5xVCoV1Gq100LeaVhcCN56MAkKmYBPcwvx4ufHeCNAIiIP51LAUSqVSEpKQnZ2tmOdzWZDdnY2UlJSmtwnJSXFqT0AbN269brtAeDChQsoLy9HVFSU4xiVlZXIyclxtNm+fTtsNht0Op0rH4G81O19I/DK7xMgCMA6/Tm8ln1K6pKIiKgduXyJKj09HW+//TbWrVuHEydO4PHHH0dNTQ1mzpwJAJg+fToyMjIc7Z944glkZWXhlVdewcmTJ/HCCy/gwIEDmDdvHgCguroaTz/9NPbs2YOzZ88iOzsb99xzD3r16oW0tDQAQP/+/TF+/HjMnj0b+/btw/fff4958+ZhypQpLZpBRQQA9yR2xYtXbgS4ctsprP3+jMQVERFRe1G4usPkyZNRWlqKxYsXw2AwIDExEVlZWY6BxAUFBZDJruamkSNHYv369Vi0aBGee+459O7dG5s3b8agQYMAAHK5HEeOHMG6detQWVmJ6Oho3HHHHfjrX/8KlerqtN73338f8+bNw7hx4yCTyTBp0iSsWrXqZj8/eZnpKXG4VFOPv2/7ES98fhy+PnJMSe4udVlERNTGXL4PTmfF++BQI1EU8dIXJ/Dv785AEICXJw3B74dxlh0RkTvqkPvgEHkCQRCw6K7+eGhkHEQRePb/juD/ci5IXRYREbUhBhzySoIgYMlvB+API2IhisBTHx3G5kN8OCcRkadgwCGvJQgCXrx7IB7QdYcoAukf5uKzw4VSl0VERG2AAYe8mkwm4KV7BmHysBjYRGDBhkP48ABvCklE1Nkx4JDXk8kEZN43GFOT7SHnmY+OYN3us1KXRUREN4EBhwj2kPO3ewdj1qh4AMCSz45h9Y7TEldFREStxYBDdEXj7Ko/jesNAFj+vzwsyzrJxzoQEXVCDDhEPyMIAtJ/3QcZE/oBAN7cmY8lnx2DlQ/oJCLqVBhwiJrw6Jie+OtE+922/6M/h3nrD6Ku3ipxVURE1FIMOETX8YcRsVg1dSiUchm+OmrA9H/vQ2WtReqyiIioBRhwiG7g7oRorHs4GYG+Cuw7W4H71+hxsfKy1GUREVEzGHCImpHSMxSbHkuBVu2L0yXVuO/N73G80CR1WUREdAMMOEQt0E+rxsd/HIk+kQEoNpnxuzW7se14sdRlERHRdTDgELVQdJAfNj02EiN7hqLGYsXs/x7AP3blcxo5EZEbYsAhcoHGzwfrHk7GtCvPr8r86iSe2nQE5gbOsCIicicMOEQu8pHL8NLEQXjx7oGQywT838ELeODtvSirNktdGhERXcGAQ9QKgiBgxsg4rJ05HIG+CuScu4S7X/8OuecrpS6NiIjAgEN0U27rHY7Nc29Fj7AuKDTW4fdr9HhvzzmOyyEikhgDDtFN6hkegM3zbkXawEhYrDYs2nwUf950GJctHJdDRCQVBhyiNqD29cGaB5OQMaEfZALw8cGLuPfN73G2rEbq0oiIvBIDDlEbEQQBj47pifcfGYGwACVOGqrw29e/w+eHC6UujYjI6zDgELWxlJ6h2DL/NgyLDUaVuQHzPziEpzcdRo25QerSiIi8BgMOUTvQanyxYc4IzP9VLwgCsCnnAn77+nc4etEodWlERF6BAYeonSjkMvz5jr74YPYIRGl88VNZDe5983v869ufYLNxlhURUXtiwCFqZyN6hOKrJ25D2sBI1FtFvPTFCfzhnb24cKlW6tKIiDwWAw5RBwjyV2LNg0l4aeIg+PrI8P3pcoxf+S0+2FfAe+YQEbUDBhyiDiIIAh4cEYusJ0ZjWGwwqs0NyPj4B8x4dz+KjJelLo+IyKMw4BB1sLiwLtj4aAoW3dUfSoUM3/xYijv+/g0+PHCeZ3OIiNoIAw6RBOQyAY/c1gNf/uk2JMQEoaquAc98dART396D/NJqqcsjIur0GHCIJNQrIgD/91gKFk7oB18fGfb8VIEJK7/Fym0/wtzARz0QEbUWAw6RxBRyGR4b0xNbnxyDMX3CYbHasHLbKUx47Vvo88ulLo+IqFNqVcBZvXo14uLi4OvrC51Oh3379t2w/aZNm9CvXz/4+vpi8ODB+PLLLx3b6uvr8eyzz2Lw4MHo0qULoqOjMX36dBQWOt/ePi4uDoIgOC1Lly5tTflEbikmxB9rZw7HGw8MRXigCj+V1mDq23uwYMMhGIx1UpdHRNSpuBxwNm7ciPT0dCxZsgQHDx5EQkIC0tLSUFJS0mT73bt3Y+rUqZg1axYOHTqEiRMnYuLEiTh69CgAoLa2FgcPHsTzzz+PgwcP4uOPP0ZeXh7uvvvua471l7/8BUVFRY5l/vz5rpZP5NYEQcBvhkRjW/oYPDiiOwQB2JxbiF+9shOrd5xGXT0vWxERtYQgujhtQ6fTYfjw4XjjjTcAADabDTExMZg/fz4WLlx4TfvJkyejpqYGW7ZscawbMWIEEhMTsWbNmibfY//+/UhOTsa5c+fQvXt3APYzOAsWLMCCBQtcKdfBZDJBo9HAaDRCrVa36hhEHe3IhUq8+Plx5Jy7BACICfHD/7tzANIGRkIQBImrIyJqf639/nbpDI7FYkFOTg5SU1OvHkAmQ2pqKvR6fZP76PV6p/YAkJaWdt32AGA0GiEIAoKCgpzWL126FKGhoRg6dCiWL1+OhobrP7zQbDbDZDI5LUSdzZBuQfjosRS8NiURWrUvzldcxmPv5WDav/byuVZERDfgUsApKyuD1WpFZGSk0/rIyEgYDIYm9zEYDC61r6urw7PPPoupU6c6JbU//elP2LBhA3bs2IFHH30Uf/vb3/DMM89ct9bMzExoNBrHEhMT09KPSeRWBEHAPYldkf3nMZh3ey8oFTLszi/Hb17/DvM/OIRz5TVSl0hE5HYUUhfwc/X19fj9738PURTx1ltvOW1LT093vB4yZAiUSiUeffRRZGZmQqVSXXOsjIwMp31MJhNDDnVqXVQKPJXWF5OHx2DF13n4NLcQnx8uxFc/FOEBXXfM/1VvhAde+7dAROSNXDqDExYWBrlcjuLiYqf1xcXF0Gq1Te6j1Wpb1L4x3Jw7dw5bt25t9jqbTqdDQ0MDzp492+R2lUoFtVrttBB5gpgQf7w2ZSi2zB+F0X3C0WAT8R/9OYxZvgOvbv0Rxsv1UpdIRCQ5lwKOUqlEUlISsrOzHetsNhuys7ORkpLS5D4pKSlO7QFg69atTu0bw82pU6ewbds2hIaGNltLbm4uZDIZIiIiXPkIRB5jUFcN/vNwMtY/okNCNw1qLVasyj6FUS9vtwedWgYdIvJeLl+iSk9Px4wZMzBs2DAkJydj5cqVqKmpwcyZMwEA06dPR9euXZGZmQkAeOKJJzBmzBi88soruOuuu7BhwwYcOHAA//znPwHYw83999+PgwcPYsuWLbBarY7xOSEhIVAqldDr9di7dy9uv/12BAYGQq/X48knn8SDDz6I4ODgtuoLok5pZK8wbJ57K746asDKbT/ix+JqrMo+hXe/O4OHbo3DrFHxCPJXSl0mEVGHcnmaOAC88cYbWL58OQwGAxITE7Fq1SrodDoAwNixYxEXF4e1a9c62m/atAmLFi3C2bNn0bt3byxbtgx33nknAODs2bOIj49v8n127NiBsWPH4uDBg/jjH/+IkydPwmw2Iz4+Hn/4wx+Qnp7e5PibpnCaOHkDm03EV0cNWJV9CnnFVQCALko5ZoyMw8Oj4hEWwDE6RNS5tPb7u1UBpzNiwCFvYrOJ+N8xA17LPoWTBnvQUSlkmJTUDY+MikeP8ACJKyQiahkGnGYw4JA3stlEbD1RjDd35uPw+UoAgCAAv+4fiUfH9EBSbIi0BRIRNYMBpxkMOOTNRFHEvjMVePvbn7DtxNXHqtzSPQizRvXAHQMj4SPns3eJyP0w4DSDAYfI7nRJFd7+5gw+OXQRFqsNABCpVuGB5FhMTY5BhNpX4gqJiK5iwGkGAw6Rs5KqOvxXfw4f7DuPsmozAEAhEzB+kBbTU+IwPC6Yz7siIskx4DSDAYeoaZYGG746WoT/6s/hwJWHegJAP20gfj8sBvcO7YrgLpxmTkTSYMBpBgMOUfOOFRrxX/05bM69iLp6++UrpVyGXw+IxO+GdcNtvcMhl/GsDhF1HAacZjDgELWcsbYenx6+iA8PnMfRiybHeq3aF/cndcP9Sd0QF9ZFwgqJyFsw4DSDAYeodY4VGrHpwAVszr2Iyp89/iGhmwZ3J3bFb4dEcWAyEbUbBpxmMOAQ3RxzgxXbjpfgwwPn8e2pUtiu/J9DEICUHqG4OyEaEwZFQePvI22hRORRGHCawYBD1HZKq8z48ocifHa4EDk/G5jsIxcwpk840gZqkdo/koOTieimMeA0gwGHqH2cr6jF50cK8VluoeOxEAAglwlIjgvB+EFa3DEwElEaPwmrJKLOigGnGQw4RO0vz1CFr44W4X/HinGiyOS0LaGbBncM1GJc/wj0jQzkPXaIqEUYcJrBgEPUsQrKa/H1cQP+d8yAA+cu4ef/p4nS+GJs33CM6ROBUb3DEKBSSFcoEbk1BpxmMOAQSae0yoytx4ux9bgB+p/KHffYAezjdobFhmBs33CM7RuBPpEBPLtDRA4MOM1gwCFyD3X1Vuz5qRw780qx68dSnCmrcdoeFqBCSs9QjOwZilt7hiEmxI+Bh8iLMeA0gwGHyD2dLavBzrwS7Mgrxd4zzmd3AKBrkB9G9gzFyF6hSOkRBq2G99wh8iYMOM1gwCFyf+YGK3ILKrE7vxy788twqKASDTbn/0XFhPhhWGwIkmKDMSwuGH0iAiHj4yOIPBYDTjMYcIg6n1pLA/afvYTd+WXQ55fj6EUjfpF3EOirwC3dgzE8LhhJsSFIiNHAX8lBy0SeggGnGQw4RJ1fVV09DhVU4sC5S8g5V4FDBZWotVid2sgEoHdEIIZ002BITBCGdNWgX1QgVAq5RFUT0c1gwGkGAw6R52mw2nCiqAoHzlXYQ8/ZSzCY6q5p5yMX0D9KjcFdNUjoFoSBXdXoFRHA0EPUCTDgNIMBh8g7FJvqcOSCEUcuVDp+XvrZQ0IbKWQCeoYHoH9UIPpFqdE/So3+2kCEB6o4a4vIjTDgNIMBh8g7iaKIC5cuO8LO4QuVOF5ogqmuocn2oV2U6BcViH5aNXpHBKDXlSXIn8/VIpICA04zGHCIqJEoiigy1uFEkQknDVU4XmTCySITzpTVXDOIuVFYgBI9wwPQMyIAvcKvBp8ojS/P+BC1IwacZjDgEFFzLlusOFVShRNFJpwoqkJ+aTXyS6pRaLx2XE+jLko5YkO7IDbU/2c/7a+j1L6cwk50kxhwmsGAQ0StVWNusIed0mqcLrEv+aU1OFtWc819en5OqZAhJtgPcaFd0D3UH7Eh/ugW7I+uwX6IDvKDxs+nAz8FUefU2u9v3iyCiKgZXVQKDOkWhCHdgpzW11ttOFdei4KKGpwtq0VBRS3OltegoLwW5y/VwtJgQ35pDfJLa5o8bqBKgeggvyuBxxddg/yv/LSviwj0hZxngIhahQGHiKiVfOQyx1icX7LaRBRWXsa58lqcq6ix/yyvwcXKyyisrENFjQVV5gbkFVchr7iqyePLZQLCA1SIVKsQofZFpFqFyEBfRKp9EaFWIVJtfx3s78NxQES/wIBDRNQO5DIBMSH+iAnxxyiEXbO91tKAwsq6K4HnMi5euvLzymIw1qHBJsJgqrtybx/jdd9LKZchPNAehMIDVQgNUCG0i9K+BKgQGqBEaBf7z2B/Jc8KkVdgwCEikoC/UnHdsz+A/QxQaZUZJVV1KDaZUWyqQ4npyusr60pMdSivscBitTmCUXMEAQj2bww/zsEnyN/HvvgpofH3gcbPB0F+9p8Kuaytu4CoXTHgEBG5IblMgFbj2+zT0y0NNpRW2wNQsbEOZdVmlNdYUF5tQXmN+cpPC8qrzbhUWw9RBCpqLKioseBUScvrCVQpoLlBAAr09UGArwKBvgoEqhRXXvsgQKVAgErBs0bU4RhwiIg6MaVCZh+UHOTXbNsGqw2XauuvCT7l1RZcqrWg8nI9jLX1qLxsQWWt/XWV2X5DxCpzA6rMDbhwqfmzRE3popQ7haAAlQLqxgB05Xd/pRz+KgX8feRXXyuvvFYq0EUph9+V1wxM1JxWBZzVq1dj+fLlMBgMSEhIwOuvv47k5OTrtt+0aROef/55nD17Fr1798bLL7+MO++807FdFEUsWbIEb7/9NiorK3HrrbfirbfeQu/evR1tKioqMH/+fHz++eeQyWSYNGkSXnvtNQQENH16l4iInCmujNUJD1S1eJ8Gqw2mugZUNhGAKmvrYbxsX6rqGlBtbvzZgKq6BlTV1aPeap9GX2OxosZiBUxt81lUCpkj+PwyGPkp5VAp5PD1kcHXRw6Vwv7zl7+rFHKofGTwvdK26X3kDFOdlMsBZ+PGjUhPT8eaNWug0+mwcuVKpKWlIS8vDxEREde03717N6ZOnYrMzEz85je/wfr16zFx4kQcPHgQgwYNAgAsW7YMq1atwrp16xAfH4/nn38eaWlpOH78OHx97adnp02bhqKiImzduhX19fWYOXMm5syZg/Xr199kFxAR0fUo5DKEdFEipEvrHlVRV29FtbkB1XVXQk9jCHIEIftZouq6Bly2WFFrsaLGcvV1raXhyk/768bbDpkbbDA32Jp8zlhbU8gE+PrIoVTIoJTL4KMQ4CO3v3ask8vgo2hcJzSxTgYfuQClXA6fK9vt6xqP2XgcAXKZ/fj2nwLkMhkUMgEKuWD/KbNvs//+820yRxvOqmvFjf50Oh2GDx+ON954AwBgs9kQExOD+fPnY+HChde0nzx5MmpqarBlyxbHuhEjRiAxMRFr1qyBKIqIjo7Gn//8Zzz11FMAAKPRiMjISKxduxZTpkzBiRMnMGDAAOzfvx/Dhg0DAGRlZeHOO+/EhQsXEB0d3WzdvNEfEVHnJooizA22a4OPucEpGF2ut6Ku3gZzg/1nXb0V5gYrzPU21DU0tc3+s3G9ud4Gi9Um9ce9KTLBHk7tgUiAojEwyQTI5QJ8HCHJ3kbeuAgCZDJc+V0GuWB/LRPsIUom/Lyd/diyK7/LZQJ6RQTgwRGxbfpZOuRGfxaLBTk5OcjIyHCsk8lkSE1NhV6vb3IfvV6P9PR0p3VpaWnYvHkzAODMmTMwGAxITU11bNdoNNDpdNDr9ZgyZQr0ej2CgoIc4QYAUlNTIZPJsHfvXtx7773XvK/ZbIbZbHb8bjK10XlRIiKShCAIjstGrT2j1FJWmwhLY/C5EobqrTZYGuzhp77xp9UGS4PoWFdvta+/2k5sYt3P24lOx22wiWiw2mC12dfbf4qw2kT7NpsNVquIetvVbU2xifYB6JZ27aVrje4T3uYBp7VcCjhlZWWwWq2IjIx0Wh8ZGYmTJ082uY/BYGiyvcFgcGxvXHejNr+8/KVQKBASEuJo80uZmZl48cUXW/jJiIiIrpLLBPhdGc/j7qxXgk+DVXQKSPbXV7ZdeW21/TwcXWl3ZT+rTYRNvPqzwSrCKoqw2a7+dG6Ha9rFhvpL3R0OHjuLKiMjw+nMkclkQkxMjIQVERERtT375SQ5VB77jd46Lt25KSwsDHK5HMXFxU7ri4uLodVqm9xHq9XesH3jz+balJQ437ChoaEBFRUV131flUoFtVrttBAREZF3cCngKJVKJCUlITs727HOZrMhOzsbKSkpTe6TkpLi1B4Atm7d6mgfHx8PrVbr1MZkMmHv3r2ONikpKaisrEROTo6jzfbt22Gz2aDT6Vz5CEREROQFXD6hlZ6ejhkzZmDYsGFITk7GypUrUVNTg5kzZwIApk+fjq5duyIzMxMA8MQTT2DMmDF45ZVXcNddd2HDhg04cOAA/vnPfwKwDxpbsGABXnrpJfTu3dsxTTw6OhoTJ04EAPTv3x/jx4/H7NmzsWbNGtTX12PevHmYMmVKi2ZQERERkXdxOeBMnjwZpaWlWLx4MQwGAxITE5GVleUYJFxQUACZ7OqJoZEjR2L9+vVYtGgRnnvuOfTu3RubN2923AMHAJ555hnU1NRgzpw5qKysxKhRo5CVleW4Bw4AvP/++5g3bx7GjRvnuNHfqlWrbuazExERkYdy+T44nRXvg0NERNT5tPb7m4+HJSIiIo/DgENEREQehwGHiIiIPA4DDhEREXkcBhwiIiLyOAw4RERE5HEYcIiIiMjjMOAQERGRx/GaZ4823s/QZDJJXAkRERG1VOP3tqv3JfaagFNVVQUAiImJkbgSIiIiclVVVRU0Gk2L23vNoxpsNhsKCwsRGBgIQRDa9NgmkwkxMTE4f/48HwPRDPZVy7GvWo591XLsK9ewv1quvfpKFEVUVVUhOjra6VmXzfGaMzgymQzdunVr1/dQq9X8A2gh9lXLsa9ajn3Vcuwr17C/Wq49+sqVMzeNOMiYiIiIPA4DDhEREXkcBpw2oFKpsGTJEqhUKqlLcXvsq5ZjX7Uc+6rl2FeuYX+1nLv1ldcMMiYiIiLvwTM4RERE5HEYcIiIiMjjMOAQERGRx2HAISIiIo/DgHOTVq9ejbi4OPj6+kKn02Hfvn1Sl9SmXnjhBQiC4LT069fPsb2urg5z585FaGgoAgICMGnSJBQXFzsdo6CgAHfddRf8/f0RERGBp59+Gg0NDU5tdu7ciVtuuQUqlQq9evXC2rVrr6nF3fr6m2++wW9/+1tER0dDEARs3rzZabsoili8eDGioqLg5+eH1NRUnDp1yqlNRUUFpk2bBrVajaCgIMyaNQvV1dVObY4cOYLbbrsNvr6+iImJwbJly66pZdOmTejXrx98fX0xePBgfPnlly7X0p6a66uHHnromn9n48ePd2rjLX2VmZmJ4cOHIzAwEBEREZg4cSLy8vKc2rjT311LamkvLemrsWPHXvNv67HHHnNq4w199dZbb2HIkCGOm/ClpKTgq6++cqm2TtdPIrXahg0bRKVSKb7zzjvisWPHxNmzZ4tBQUFicXGx1KW1mSVLlogDBw4Ui4qKHEtpaalj+2OPPSbGxMSI2dnZ4oEDB8QRI0aII0eOdGxvaGgQBw0aJKampoqHDh0Sv/zySzEsLEzMyMhwtPnpp59Ef39/MT09XTx+/Lj4+uuvi3K5XMzKynK0cce+/vLLL8X/9//+n/jxxx+LAMRPPvnEafvSpUtFjUYjbt68WTx8+LB49913i/Hx8eLly5cdbcaPHy8mJCSIe/bsEb/99luxV69e4tSpUx3bjUajGBkZKU6bNk08evSo+MEHH4h+fn7iP/7xD0eb77//XpTL5eKyZcvE48ePi4sWLRJ9fHzEH374waVa2lNzfTVjxgxx/PjxTv/OKioqnNp4S1+lpaWJ7777rnj06FExNzdXvPPOO8Xu3buL1dXVjjbu9HfXXC3tqSV9NWbMGHH27NlO/7aMRqNju7f01WeffSZ+8cUX4o8//ijm5eWJzz33nOjj4yMePXq0RbV1xn5iwLkJycnJ4ty5cx2/W61WMTo6WszMzJSwqra1ZMkSMSEhocltlZWVoo+Pj7hp0ybHuhMnTogARL1eL4qi/YtNJpOJBoPB0eatt94S1Wq1aDabRVEUxWeeeUYcOHCg07EnT54spqWlOX53977+5Ze2zWYTtVqtuHz5cse6yspKUaVSiR988IEoiqJ4/PhxEYC4f/9+R5uvvvpKFARBvHjxoiiKovjmm2+KwcHBjr4SRVF89tlnxb59+zp+//3vfy/eddddTvXodDrx0UcfbXEtHel6Aeeee+657j7e2leiKIolJSUiAHHXrl2Oetzl764ltXSkX/aVKNoDzhNPPHHdfby1r0RRFIODg8V//etfHvtvipeoWslisSAnJwepqamOdTKZDKmpqdDr9RJW1vZOnTqF6Oho9OjRA9OmTUNBQQEAICcnB/X19U590K9fP3Tv3t3RB3q9HoMHD0ZkZKSjTVpaGkwmE44dO+Zo8/NjNLZpPEZn7OszZ87AYDA41azRaKDT6Zz6JigoCMOGDXO0SU1NhUwmw969ex1tRo8eDaVS6WiTlpaGvLw8XLp0ydHmRv3Xklrcwc6dOxEREYG+ffvi8ccfR3l5uWObN/eV0WgEAISEhABwr7+7ltTSkX7ZV43ef/99hIWFYdCgQcjIyEBtba1jmzf2ldVqxYYNG1BTU4OUlBSP/TflNQ/bbGtlZWWwWq1O/7EBIDIyEidPnpSoqran0+mwdu1a9O3bF0VFRXjxxRdx22234ejRozAYDFAqlQgKCnLaJzIyEgaDAQBgMBia7KPGbTdqYzKZcPnyZVy6dKnT9XXjZ2uq5p9/7oiICKftCoUCISEhTm3i4+OvOUbjtuDg4Ov238+P0VwtUhs/fjzuu+8+xMfHIz8/H8899xwmTJgAvV4PuVzutX1ls9mwYMEC3HrrrRg0aJCjRnf5u2tJLR2lqb4CgAceeACxsbGIjo7GkSNH8OyzzyIvLw8ff/wxAO/qqx9++AEpKSmoq6tDQEAAPvnkEwwYMAC5ubke+W+KAYduaMKECY7XQ4YMgU6nQ2xsLD788EP4+flJWBl5kilTpjheDx48GEOGDEHPnj2xc+dOjBs3TsLKpDV37lwcPXoU3333ndSluL3r9dWcOXMcrwcPHoyoqCiMGzcO+fn56NmzZ0eXKam+ffsiNzcXRqMRH330EWbMmIFdu3ZJXVa74SWqVgoLC4NcLr9mZHdxcTG0Wq1EVbW/oKAg9OnTB6dPn4ZWq4XFYkFlZaVTm5/3gVarbbKPGrfdqI1arYafn1+n7OvGum5Us1arRUlJidP2hoYGVFRUtEn//Xx7c7W4mx49eiAsLAynT58G4J19NW/ePGzZsgU7duxAt27dHOvd6e+uJbV0hOv1VVN0Oh0AOP3b8pa+UiqV6NWrF5KSkpCZmYmEhAS89tprHvtvigGnlZRKJZKSkpCdne1YZ7PZkJ2djZSUFAkra1/V1dXIz89HVFQUkpKS4OPj49QHeXl5KCgocPRBSkoKfvjhB6cvp61bt0KtVmPAgAGONj8/RmObxmN0xr6Oj4+HVqt1qtlkMmHv3r1OfVNZWYmcnBxHm+3bt8Nmszn+J5ySkoJvvvkG9fX1jjZbt25F3759ERwc7Ghzo/5rSS3u5sKFCygvL0dUVBQA7+orURQxb948fPLJJ9i+ffs1l93c6e+uJbW0p+b6qim5ubkA4PRvyxv6qik2mw1ms9lz/025NCSZnGzYsEFUqVTi2rVrxePHj4tz5swRg4KCnEaZd3Z//vOfxZ07d4pnzpwRv//+ezE1NVUMCwsTS0pKRFG0T+fr3r27uH37dvHAgQNiSkqKmJKS4ti/cWrhHXfcIebm5opZWVlieHh4k1MLn376afHEiRPi6tWrm5xa6G59XVVVJR46dEg8dOiQCEB89dVXxUOHDonnzp0TRdE+3TgoKEj89NNPxSNHjoj33HNPk9PEhw4dKu7du1f87rvvxN69eztNfa6srBQjIyPFP/zhD+LRo0fFDRs2iP7+/tdMfVYoFOKKFSvEEydOiEuWLGly6nNztbSnG/VVVVWV+NRTT4l6vV48c+aMuG3bNvGWW24Re/fuLdbV1XldXz3++OOiRqMRd+7c6TS1uba21tHGnf7umqulPTXXV6dPnxb/8pe/iAcOHBDPnDkjfvrpp2KPHj3E0aNHO47hLX21cOFCcdeuXeKZM2fEI0eOiAsXLhQFQRC//vrrFtXWGfuJAecmvf7662L37t1FpVIpJicni3v27JG6pDY1efJkMSoqSlQqlWLXrl3FyZMni6dPn3Zsv3z5svjHP/5RDA4OFv39/cV7771XLCoqcjrG2bNnxQkTJoh+fn5iWFiY+Oc//1msr693arNjxw4xMTFRVCqVYo8ePcR33333mlrcra937NghArhmmTFjhiiK9inHzz//vBgZGSmqVCpx3LhxYl5entMxysvLxalTp4oBAQGiWq0WZ86cKVZVVTm1OXz4sDhq1ChRpVKJXbt2FZcuXXpNLR9++KHYp08fUalUigMHDhS/+OILp+0tqaU93aivamtrxTvuuEMMDw8XfXx8xNjYWHH27NnXhFdv6aum+gmA09+EO/3dtaSW9tJcXxUUFIijR48WQ0JCRJVKJfbq1Ut8+umnne6DI4re0VcPP/ywGBsbKyqVSjE8PFwcN26cI9y0tLbO1k+CKIqia+d8iIiIiNwbx+AQERGRx2HAISIiIo/DgENEREQehwGHiIiIPA4DDhEREXkcBhwiIiLyOAw4RERE5HEYcIiIiMjjMOAQERGRx2HAISIiIo/DgENEREQehwGHiIiIPM7/D9ymVANBflMJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "timesteps=300000\n",
    "init_lr = 0.2\n",
    "decay_rate = 0.00002\n",
    "plt.plot(np.arange(timesteps), [init_lr * ((1 - decay_rate) ** i)for i in range(1, timesteps + 1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E01: \n",
    "#### *Tune the hyperparameters of the training to beat my best validation loss of 2.2*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running many different experiments, I was able to improve upon the results in lecture by increasing the minibatch size, and using learning rate decay to gradually reduce the learning rate over the course of training. I think that using a larger minibatch size resulted in smoother convergence than the size 32 batches we used in lecture, which helped us reach a lower loss value. A smooth learning rate decay curve also likely helped with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 22.539005279541016\n",
      "Iteration 500 loss: 4.827736854553223\n",
      "Iteration 1000 loss: 3.2072134017944336\n",
      "Iteration 1500 loss: 2.9704630374908447\n",
      "Iteration 2000 loss: 2.3785486221313477\n",
      "Iteration 2500 loss: 2.5785391330718994\n",
      "Iteration 3000 loss: 2.7617363929748535\n",
      "Iteration 3500 loss: 2.5517711639404297\n",
      "Iteration 4000 loss: 2.4682247638702393\n",
      "Iteration 4500 loss: 2.4683382511138916\n",
      "Iteration 5000 loss: 2.4201836585998535\n",
      "Iteration 5500 loss: 2.4366612434387207\n",
      "Iteration 6000 loss: 2.4743969440460205\n",
      "Iteration 6500 loss: 2.5311696529388428\n",
      "Iteration 7000 loss: 2.255258083343506\n",
      "Iteration 7500 loss: 2.360548734664917\n",
      "Iteration 8000 loss: 2.2804486751556396\n",
      "Iteration 8500 loss: 2.407033681869507\n",
      "Iteration 9000 loss: 2.24334454536438\n",
      "Iteration 9500 loss: 2.4069130420684814\n",
      "Iteration 10000 loss: 2.451526641845703\n",
      "Iteration 10500 loss: 2.357257604598999\n",
      "Iteration 11000 loss: 2.318587064743042\n",
      "Iteration 11500 loss: 2.4463751316070557\n",
      "Iteration 12000 loss: 2.4933009147644043\n",
      "Iteration 12500 loss: 2.434859037399292\n",
      "Iteration 13000 loss: 2.2950360774993896\n",
      "Iteration 13500 loss: 2.2058091163635254\n",
      "Iteration 14000 loss: 2.2254536151885986\n",
      "Iteration 14500 loss: 2.413851737976074\n",
      "Iteration 15000 loss: 2.2997210025787354\n",
      "Iteration 15500 loss: 2.465181589126587\n",
      "Iteration 16000 loss: 2.2097606658935547\n",
      "Iteration 16500 loss: 2.4168288707733154\n",
      "Iteration 17000 loss: 2.143246650695801\n",
      "Iteration 17500 loss: 2.434915781021118\n",
      "Iteration 18000 loss: 2.40531063079834\n",
      "Iteration 18500 loss: 2.2545132637023926\n",
      "Iteration 19000 loss: 2.4309966564178467\n",
      "Iteration 19500 loss: 2.4677209854125977\n",
      "Iteration 20000 loss: 2.2597861289978027\n",
      "Iteration 20500 loss: 2.2503204345703125\n",
      "Iteration 21000 loss: 2.5247819423675537\n",
      "Iteration 21500 loss: 2.272705078125\n",
      "Iteration 22000 loss: 2.2565531730651855\n",
      "Iteration 22500 loss: 2.332432508468628\n",
      "Iteration 23000 loss: 2.2856976985931396\n",
      "Iteration 23500 loss: 2.4984564781188965\n",
      "Iteration 24000 loss: 2.1186397075653076\n",
      "Iteration 24500 loss: 2.1803693771362305\n",
      "Iteration 25000 loss: 2.324176788330078\n",
      "Iteration 25500 loss: 2.2463173866271973\n",
      "Iteration 26000 loss: 2.350081205368042\n",
      "Iteration 26500 loss: 2.180905818939209\n",
      "Iteration 27000 loss: 2.199403762817383\n",
      "Iteration 27500 loss: 2.3567755222320557\n",
      "Iteration 28000 loss: 2.374753713607788\n",
      "Iteration 28500 loss: 2.3238015174865723\n",
      "Iteration 29000 loss: 2.1219537258148193\n",
      "Iteration 29500 loss: 2.5602850914001465\n",
      "Iteration 30000 loss: 2.396378517150879\n",
      "Iteration 30500 loss: 2.2422432899475098\n",
      "Iteration 31000 loss: 2.326529026031494\n",
      "Iteration 31500 loss: 2.484975576400757\n",
      "Iteration 32000 loss: 2.1836891174316406\n",
      "Iteration 32500 loss: 2.426175594329834\n",
      "Iteration 33000 loss: 2.3337647914886475\n",
      "Iteration 33500 loss: 2.2103452682495117\n",
      "Iteration 34000 loss: 2.3001301288604736\n",
      "Iteration 34500 loss: 2.2612714767456055\n",
      "Iteration 35000 loss: 2.13535213470459\n",
      "Iteration 35500 loss: 2.2413928508758545\n",
      "Iteration 36000 loss: 2.1335465908050537\n",
      "Iteration 36500 loss: 2.5038630962371826\n",
      "Iteration 37000 loss: 2.2340505123138428\n",
      "Iteration 37500 loss: 2.079974889755249\n",
      "Iteration 38000 loss: 2.210651397705078\n",
      "Iteration 38500 loss: 2.3014233112335205\n",
      "Iteration 39000 loss: 2.258178472518921\n",
      "Iteration 39500 loss: 2.379469871520996\n",
      "Iteration 40000 loss: 2.2895326614379883\n",
      "Iteration 40500 loss: 2.49318528175354\n",
      "Iteration 41000 loss: 2.1617493629455566\n",
      "Iteration 41500 loss: 2.088062286376953\n",
      "Iteration 42000 loss: 2.3187825679779053\n",
      "Iteration 42500 loss: 2.2744836807250977\n",
      "Iteration 43000 loss: 2.2054667472839355\n",
      "Iteration 43500 loss: 2.343312978744507\n",
      "Iteration 44000 loss: 2.4565162658691406\n",
      "Iteration 44500 loss: 2.4584994316101074\n",
      "Iteration 45000 loss: 2.2675492763519287\n",
      "Iteration 45500 loss: 2.3382763862609863\n",
      "Iteration 46000 loss: 2.2376129627227783\n",
      "Iteration 46500 loss: 2.0744810104370117\n",
      "Iteration 47000 loss: 2.243518352508545\n",
      "Iteration 47500 loss: 1.9508153200149536\n",
      "Iteration 48000 loss: 2.295790672302246\n",
      "Iteration 48500 loss: 2.1902856826782227\n",
      "Iteration 49000 loss: 2.1982243061065674\n",
      "Iteration 49500 loss: 2.34628963470459\n",
      "Iteration 50000 loss: 2.3441247940063477\n",
      "Iteration 50500 loss: 2.174492835998535\n",
      "Iteration 51000 loss: 2.2419116497039795\n",
      "Iteration 51500 loss: 2.3288733959198\n",
      "Iteration 52000 loss: 2.0253450870513916\n",
      "Iteration 52500 loss: 2.102247714996338\n",
      "Iteration 53000 loss: 2.1704318523406982\n",
      "Iteration 53500 loss: 2.1221542358398438\n",
      "Iteration 54000 loss: 2.1842780113220215\n",
      "Iteration 54500 loss: 2.465559720993042\n",
      "Iteration 55000 loss: 2.1653475761413574\n",
      "Iteration 55500 loss: 2.2797348499298096\n",
      "Iteration 56000 loss: 2.3032045364379883\n",
      "Iteration 56500 loss: 2.3079445362091064\n",
      "Iteration 57000 loss: 2.4033889770507812\n",
      "Iteration 57500 loss: 2.4174656867980957\n",
      "Iteration 58000 loss: 2.3240928649902344\n",
      "Iteration 58500 loss: 2.2923667430877686\n",
      "Iteration 59000 loss: 2.339630126953125\n",
      "Iteration 59500 loss: 2.1749935150146484\n",
      "Iteration 60000 loss: 2.3543944358825684\n",
      "Iteration 60500 loss: 2.2019431591033936\n",
      "Iteration 61000 loss: 2.2312724590301514\n",
      "Iteration 61500 loss: 2.2926814556121826\n",
      "Iteration 62000 loss: 2.221372365951538\n",
      "Iteration 62500 loss: 2.198446035385132\n",
      "Iteration 63000 loss: 2.426948070526123\n",
      "Iteration 63500 loss: 2.02071213722229\n",
      "Iteration 64000 loss: 2.198903799057007\n",
      "Iteration 64500 loss: 2.313185453414917\n",
      "Iteration 65000 loss: 2.1973764896392822\n",
      "Iteration 65500 loss: 2.12615966796875\n",
      "Iteration 66000 loss: 2.3719804286956787\n",
      "Iteration 66500 loss: 2.2248783111572266\n",
      "Iteration 67000 loss: 2.157883644104004\n",
      "Iteration 67500 loss: 2.295893669128418\n",
      "Iteration 68000 loss: 2.1351816654205322\n",
      "Iteration 68500 loss: 2.183220148086548\n",
      "Iteration 69000 loss: 2.512535333633423\n",
      "Iteration 69500 loss: 2.1950762271881104\n",
      "Iteration 70000 loss: 2.125037431716919\n",
      "Iteration 70500 loss: 2.1306402683258057\n",
      "Iteration 71000 loss: 2.070779800415039\n",
      "Iteration 71500 loss: 2.0754549503326416\n",
      "Iteration 72000 loss: 2.3062968254089355\n",
      "Iteration 72500 loss: 2.1416401863098145\n",
      "Iteration 73000 loss: 2.2823288440704346\n",
      "Iteration 73500 loss: 2.359069347381592\n",
      "Iteration 74000 loss: 2.166738271713257\n",
      "Iteration 74500 loss: 2.2350354194641113\n",
      "Iteration 75000 loss: 2.1918344497680664\n",
      "Iteration 75500 loss: 2.294562578201294\n",
      "Iteration 76000 loss: 2.268582820892334\n",
      "Iteration 76500 loss: 2.2404751777648926\n",
      "Iteration 77000 loss: 2.2517197132110596\n",
      "Iteration 77500 loss: 2.2526040077209473\n",
      "Iteration 78000 loss: 2.169281244277954\n",
      "Iteration 78500 loss: 2.108996629714966\n",
      "Iteration 79000 loss: 2.0434107780456543\n",
      "Iteration 79500 loss: 2.0793564319610596\n",
      "Iteration 80000 loss: 2.159911632537842\n",
      "Iteration 80500 loss: 2.2302358150482178\n",
      "Iteration 81000 loss: 2.32505464553833\n",
      "Iteration 81500 loss: 2.156981945037842\n",
      "Iteration 82000 loss: 2.2099549770355225\n",
      "Iteration 82500 loss: 2.233222007751465\n",
      "Iteration 83000 loss: 2.204918146133423\n",
      "Iteration 83500 loss: 2.3314216136932373\n",
      "Iteration 84000 loss: 2.230173110961914\n",
      "Iteration 84500 loss: 2.2357444763183594\n",
      "Iteration 85000 loss: 2.207864761352539\n",
      "Iteration 85500 loss: 2.4993274211883545\n",
      "Iteration 86000 loss: 2.1691901683807373\n",
      "Iteration 86500 loss: 2.1567940711975098\n",
      "Iteration 87000 loss: 2.312750816345215\n",
      "Iteration 87500 loss: 2.343155860900879\n",
      "Iteration 88000 loss: 2.1599342823028564\n",
      "Iteration 88500 loss: 2.0879528522491455\n",
      "Iteration 89000 loss: 2.182724952697754\n",
      "Iteration 89500 loss: 2.1890575885772705\n",
      "Iteration 90000 loss: 2.266772747039795\n",
      "Iteration 90500 loss: 2.183854103088379\n",
      "Iteration 91000 loss: 2.345515727996826\n",
      "Iteration 91500 loss: 2.461789846420288\n",
      "Iteration 92000 loss: 2.3067474365234375\n",
      "Iteration 92500 loss: 2.3707258701324463\n",
      "Iteration 93000 loss: 2.3207788467407227\n",
      "Iteration 93500 loss: 2.006160020828247\n",
      "Iteration 94000 loss: 2.040390968322754\n",
      "Iteration 94500 loss: 2.1914889812469482\n",
      "Iteration 95000 loss: 2.3215041160583496\n",
      "Iteration 95500 loss: 2.138383626937866\n",
      "Iteration 96000 loss: 2.383587121963501\n",
      "Iteration 96500 loss: 2.271860122680664\n",
      "Iteration 97000 loss: 2.168748140335083\n",
      "Iteration 97500 loss: 2.121680498123169\n",
      "Iteration 98000 loss: 2.351668119430542\n",
      "Iteration 98500 loss: 1.9771345853805542\n",
      "Iteration 99000 loss: 2.1260108947753906\n",
      "Iteration 99500 loss: 2.104724168777466\n",
      "Iteration 100000 loss: 2.2495615482330322\n",
      "Iteration 100500 loss: 2.2436394691467285\n",
      "Iteration 101000 loss: 2.2324488162994385\n",
      "Iteration 101500 loss: 1.9613960981369019\n",
      "Iteration 102000 loss: 2.1861157417297363\n",
      "Iteration 102500 loss: 2.4153213500976562\n",
      "Iteration 103000 loss: 2.2406227588653564\n",
      "Iteration 103500 loss: 2.0140252113342285\n",
      "Iteration 104000 loss: 2.2142505645751953\n",
      "Iteration 104500 loss: 2.1661689281463623\n",
      "Iteration 105000 loss: 2.097116231918335\n",
      "Iteration 105500 loss: 2.1572318077087402\n",
      "Iteration 106000 loss: 2.4609079360961914\n",
      "Iteration 106500 loss: 2.2970523834228516\n",
      "Iteration 107000 loss: 2.253087043762207\n",
      "Iteration 107500 loss: 2.1312384605407715\n",
      "Iteration 108000 loss: 2.0331974029541016\n",
      "Iteration 108500 loss: 2.2988059520721436\n",
      "Iteration 109000 loss: 2.2755250930786133\n",
      "Iteration 109500 loss: 2.1820621490478516\n",
      "Iteration 110000 loss: 2.1273152828216553\n",
      "Iteration 110500 loss: 2.152806520462036\n",
      "Iteration 111000 loss: 2.2126922607421875\n",
      "Iteration 111500 loss: 2.1406447887420654\n",
      "Iteration 112000 loss: 2.2093188762664795\n",
      "Iteration 112500 loss: 2.2531638145446777\n",
      "Iteration 113000 loss: 2.1977412700653076\n",
      "Iteration 113500 loss: 2.275120735168457\n",
      "Iteration 114000 loss: 2.3284482955932617\n",
      "Iteration 114500 loss: 2.2569310665130615\n",
      "Iteration 115000 loss: 2.042789936065674\n",
      "Iteration 115500 loss: 1.9961344003677368\n",
      "Iteration 116000 loss: 2.177781581878662\n",
      "Iteration 116500 loss: 2.31984281539917\n",
      "Iteration 117000 loss: 1.890288233757019\n",
      "Iteration 117500 loss: 2.0643208026885986\n",
      "Iteration 118000 loss: 2.14654541015625\n",
      "Iteration 118500 loss: 1.9873474836349487\n",
      "Iteration 119000 loss: 2.034851312637329\n",
      "Iteration 119500 loss: 1.9257193803787231\n",
      "Iteration 120000 loss: 2.3114354610443115\n",
      "Iteration 120500 loss: 2.170027494430542\n",
      "Iteration 121000 loss: 2.0252416133880615\n",
      "Iteration 121500 loss: 2.256129503250122\n",
      "Iteration 122000 loss: 2.143101215362549\n",
      "Iteration 122500 loss: 2.214212656021118\n",
      "Iteration 123000 loss: 2.391173839569092\n",
      "Iteration 123500 loss: 2.3470618724823\n",
      "Iteration 124000 loss: 2.153949022293091\n",
      "Iteration 124500 loss: 2.0453357696533203\n",
      "Iteration 125000 loss: 2.2180333137512207\n",
      "Iteration 125500 loss: 1.9552687406539917\n",
      "Iteration 126000 loss: 2.0931379795074463\n",
      "Iteration 126500 loss: 2.1832497119903564\n",
      "Iteration 127000 loss: 2.110863447189331\n",
      "Iteration 127500 loss: 2.256988286972046\n",
      "Iteration 128000 loss: 2.1780214309692383\n",
      "Iteration 128500 loss: 2.026754379272461\n",
      "Iteration 129000 loss: 1.980138897895813\n",
      "Iteration 129500 loss: 2.117365598678589\n",
      "Iteration 130000 loss: 2.240783452987671\n",
      "Iteration 130500 loss: 2.1279869079589844\n",
      "Iteration 131000 loss: 2.346846342086792\n",
      "Iteration 131500 loss: 2.2363851070404053\n",
      "Iteration 132000 loss: 2.0303468704223633\n",
      "Iteration 132500 loss: 2.3179593086242676\n",
      "Iteration 133000 loss: 2.068464756011963\n",
      "Iteration 133500 loss: 2.1582398414611816\n",
      "Iteration 134000 loss: 2.0397281646728516\n",
      "Iteration 134500 loss: 2.1695597171783447\n",
      "Iteration 135000 loss: 2.1153347492218018\n",
      "Iteration 135500 loss: 1.99118971824646\n",
      "Iteration 136000 loss: 2.182185173034668\n",
      "Iteration 136500 loss: 2.068639039993286\n",
      "Iteration 137000 loss: 2.1136293411254883\n",
      "Iteration 137500 loss: 2.197291612625122\n",
      "Iteration 138000 loss: 2.207775354385376\n",
      "Iteration 138500 loss: 2.314929246902466\n",
      "Iteration 139000 loss: 2.2152483463287354\n",
      "Iteration 139500 loss: 2.152637243270874\n",
      "Iteration 140000 loss: 2.120748281478882\n",
      "Iteration 140500 loss: 2.1494224071502686\n",
      "Iteration 141000 loss: 2.208296298980713\n",
      "Iteration 141500 loss: 1.961326241493225\n",
      "Iteration 142000 loss: 2.319178819656372\n",
      "Iteration 142500 loss: 2.276939630508423\n",
      "Iteration 143000 loss: 2.112713575363159\n",
      "Iteration 143500 loss: 2.2644758224487305\n",
      "Iteration 144000 loss: 2.2519376277923584\n",
      "Iteration 144500 loss: 2.0701732635498047\n",
      "Iteration 145000 loss: 2.0675227642059326\n",
      "Iteration 145500 loss: 2.1666107177734375\n",
      "Iteration 146000 loss: 2.3777267932891846\n",
      "Iteration 146500 loss: 2.0939455032348633\n",
      "Iteration 147000 loss: 2.162451982498169\n",
      "Iteration 147500 loss: 2.160923719406128\n",
      "Iteration 148000 loss: 2.1054863929748535\n",
      "Iteration 148500 loss: 1.969830870628357\n",
      "Iteration 149000 loss: 2.2575714588165283\n",
      "Iteration 149500 loss: 2.0983519554138184\n",
      "Iteration 150000 loss: 2.169172525405884\n",
      "Iteration 150500 loss: 2.028376340866089\n",
      "Iteration 151000 loss: 2.1779656410217285\n",
      "Iteration 151500 loss: 2.212742328643799\n",
      "Iteration 152000 loss: 1.9218353033065796\n",
      "Iteration 152500 loss: 2.028245449066162\n",
      "Iteration 153000 loss: 2.1713433265686035\n",
      "Iteration 153500 loss: 2.300654649734497\n",
      "Iteration 154000 loss: 2.088925838470459\n",
      "Iteration 154500 loss: 2.2095112800598145\n",
      "Iteration 155000 loss: 2.297197103500366\n",
      "Iteration 155500 loss: 2.0358331203460693\n",
      "Iteration 156000 loss: 2.1435019969940186\n",
      "Iteration 156500 loss: 2.053814649581909\n",
      "Iteration 157000 loss: 2.150230646133423\n",
      "Iteration 157500 loss: 2.068887710571289\n",
      "Iteration 158000 loss: 2.0439839363098145\n",
      "Iteration 158500 loss: 1.9771924018859863\n",
      "Iteration 159000 loss: 2.219975471496582\n",
      "Iteration 159500 loss: 2.320910930633545\n",
      "Iteration 160000 loss: 2.208954095840454\n",
      "Iteration 160500 loss: 2.2862708568573\n",
      "Iteration 161000 loss: 2.0611965656280518\n",
      "Iteration 161500 loss: 2.149893283843994\n",
      "Iteration 162000 loss: 2.4279351234436035\n",
      "Iteration 162500 loss: 2.1806297302246094\n",
      "Iteration 163000 loss: 2.143240451812744\n",
      "Iteration 163500 loss: 1.9945892095565796\n",
      "Iteration 164000 loss: 2.2819736003875732\n",
      "Iteration 164500 loss: 2.005053758621216\n",
      "Iteration 165000 loss: 2.205047130584717\n",
      "Iteration 165500 loss: 2.225414514541626\n",
      "Iteration 166000 loss: 2.2775797843933105\n",
      "Iteration 166500 loss: 1.942983627319336\n",
      "Iteration 167000 loss: 2.065274953842163\n",
      "Iteration 167500 loss: 2.220710515975952\n",
      "Iteration 168000 loss: 2.171377182006836\n",
      "Iteration 168500 loss: 2.0607399940490723\n",
      "Iteration 169000 loss: 2.2340073585510254\n",
      "Iteration 169500 loss: 2.3037784099578857\n",
      "Iteration 170000 loss: 2.097363233566284\n",
      "Iteration 170500 loss: 2.1323788166046143\n",
      "Iteration 171000 loss: 2.0576553344726562\n",
      "Iteration 171500 loss: 2.1472854614257812\n",
      "Iteration 172000 loss: 2.186274290084839\n",
      "Iteration 172500 loss: 2.253378391265869\n",
      "Iteration 173000 loss: 2.1513547897338867\n",
      "Iteration 173500 loss: 2.0691254138946533\n",
      "Iteration 174000 loss: 2.2948763370513916\n",
      "Iteration 174500 loss: 1.8845528364181519\n",
      "Iteration 175000 loss: 2.1576919555664062\n",
      "Iteration 175500 loss: 2.246805429458618\n",
      "Iteration 176000 loss: 2.3361830711364746\n",
      "Iteration 176500 loss: 2.1999435424804688\n",
      "Iteration 177000 loss: 2.0580127239227295\n",
      "Iteration 177500 loss: 2.187177896499634\n",
      "Iteration 178000 loss: 1.9294483661651611\n",
      "Iteration 178500 loss: 2.160395622253418\n",
      "Iteration 179000 loss: 2.1650917530059814\n",
      "Iteration 179500 loss: 2.052562952041626\n",
      "Iteration 180000 loss: 1.9259130954742432\n",
      "Iteration 180500 loss: 2.1713435649871826\n",
      "Iteration 181000 loss: 2.316922903060913\n",
      "Iteration 181500 loss: 2.073237657546997\n",
      "Iteration 182000 loss: 2.254499673843384\n",
      "Iteration 182500 loss: 2.302839756011963\n",
      "Iteration 183000 loss: 2.1298704147338867\n",
      "Iteration 183500 loss: 2.1358280181884766\n",
      "Iteration 184000 loss: 2.37931227684021\n",
      "Iteration 184500 loss: 2.065093755722046\n",
      "Iteration 185000 loss: 2.335987091064453\n",
      "Iteration 185500 loss: 2.0608410835266113\n",
      "Iteration 186000 loss: 2.2262282371520996\n",
      "Iteration 186500 loss: 2.1032447814941406\n",
      "Iteration 187000 loss: 2.1337499618530273\n",
      "Iteration 187500 loss: 2.0998260974884033\n",
      "Iteration 188000 loss: 2.1148014068603516\n",
      "Iteration 188500 loss: 2.140376091003418\n",
      "Iteration 189000 loss: 2.161487579345703\n",
      "Iteration 189500 loss: 2.184448003768921\n",
      "Iteration 190000 loss: 2.1526310443878174\n",
      "Iteration 190500 loss: 2.186669111251831\n",
      "Iteration 191000 loss: 1.9351372718811035\n",
      "Iteration 191500 loss: 2.2723124027252197\n",
      "Iteration 192000 loss: 2.0756001472473145\n",
      "Iteration 192500 loss: 2.1779274940490723\n",
      "Iteration 193000 loss: 2.1393353939056396\n",
      "Iteration 193500 loss: 2.0890250205993652\n",
      "Iteration 194000 loss: 1.9949159622192383\n",
      "Iteration 194500 loss: 2.0081515312194824\n",
      "Iteration 195000 loss: 1.9645252227783203\n",
      "Iteration 195500 loss: 2.1554055213928223\n",
      "Iteration 196000 loss: 2.1886961460113525\n",
      "Iteration 196500 loss: 2.3625545501708984\n",
      "Iteration 197000 loss: 2.273028612136841\n",
      "Iteration 197500 loss: 2.257282018661499\n",
      "Iteration 198000 loss: 2.266911029815674\n",
      "Iteration 198500 loss: 2.107286214828491\n",
      "Iteration 199000 loss: 2.2129135131835938\n",
      "Iteration 199500 loss: 2.2542240619659424\n",
      "Iteration 200000 loss: 2.209364175796509\n",
      "Iteration 200500 loss: 2.100525140762329\n",
      "Iteration 201000 loss: 2.1630234718322754\n",
      "Iteration 201500 loss: 2.0104522705078125\n",
      "Iteration 202000 loss: 2.0870325565338135\n",
      "Iteration 202500 loss: 2.0763416290283203\n",
      "Iteration 203000 loss: 1.9766464233398438\n",
      "Iteration 203500 loss: 2.1006715297698975\n",
      "Iteration 204000 loss: 2.2053258419036865\n",
      "Iteration 204500 loss: 2.1306519508361816\n",
      "Iteration 205000 loss: 2.0527679920196533\n",
      "Iteration 205500 loss: 2.2562358379364014\n",
      "Iteration 206000 loss: 2.1579201221466064\n",
      "Iteration 206500 loss: 2.151311159133911\n",
      "Iteration 207000 loss: 2.368647336959839\n",
      "Iteration 207500 loss: 2.060140371322632\n",
      "Iteration 208000 loss: 2.2356505393981934\n",
      "Iteration 208500 loss: 2.152808666229248\n",
      "Iteration 209000 loss: 2.09952449798584\n",
      "Iteration 209500 loss: 2.1345090866088867\n",
      "Iteration 210000 loss: 2.224167585372925\n",
      "Iteration 210500 loss: 2.190847396850586\n",
      "Iteration 211000 loss: 2.0480799674987793\n",
      "Iteration 211500 loss: 2.1154096126556396\n",
      "Iteration 212000 loss: 2.115194797515869\n",
      "Iteration 212500 loss: 2.0702261924743652\n",
      "Iteration 213000 loss: 2.259993553161621\n",
      "Iteration 213500 loss: 1.9967013597488403\n",
      "Iteration 214000 loss: 2.1393816471099854\n",
      "Iteration 214500 loss: 2.537489652633667\n",
      "Iteration 215000 loss: 2.222583293914795\n",
      "Iteration 215500 loss: 2.347355604171753\n",
      "Iteration 216000 loss: 2.243558883666992\n",
      "Iteration 216500 loss: 2.0630109310150146\n",
      "Iteration 217000 loss: 2.1650314331054688\n",
      "Iteration 217500 loss: 2.059767723083496\n",
      "Iteration 218000 loss: 2.2324554920196533\n",
      "Iteration 218500 loss: 2.1669533252716064\n",
      "Iteration 219000 loss: 2.107417583465576\n",
      "Iteration 219500 loss: 2.110403060913086\n",
      "Iteration 220000 loss: 1.9219497442245483\n",
      "Iteration 220500 loss: 2.103492021560669\n",
      "Iteration 221000 loss: 2.181593418121338\n",
      "Iteration 221500 loss: 2.105658769607544\n",
      "Iteration 222000 loss: 2.2116825580596924\n",
      "Iteration 222500 loss: 2.2583141326904297\n",
      "Iteration 223000 loss: 2.1079251766204834\n",
      "Iteration 223500 loss: 2.0968570709228516\n",
      "Iteration 224000 loss: 2.0045313835144043\n",
      "Iteration 224500 loss: 2.1686134338378906\n",
      "Iteration 225000 loss: 1.9287930727005005\n",
      "Iteration 225500 loss: 2.0686614513397217\n",
      "Iteration 226000 loss: 2.1355366706848145\n",
      "Iteration 226500 loss: 2.1366233825683594\n",
      "Iteration 227000 loss: 2.06130313873291\n",
      "Iteration 227500 loss: 2.3343346118927\n",
      "Iteration 228000 loss: 2.1862967014312744\n",
      "Iteration 228500 loss: 2.0846786499023438\n",
      "Iteration 229000 loss: 2.1633081436157227\n",
      "Iteration 229500 loss: 2.154271364212036\n",
      "Iteration 230000 loss: 2.1375749111175537\n",
      "Iteration 230500 loss: 2.082512617111206\n",
      "Iteration 231000 loss: 2.0528647899627686\n",
      "Iteration 231500 loss: 2.0169920921325684\n",
      "Iteration 232000 loss: 2.0731406211853027\n",
      "Iteration 232500 loss: 2.1815967559814453\n",
      "Iteration 233000 loss: 2.1960878372192383\n",
      "Iteration 233500 loss: 1.9793457984924316\n",
      "Iteration 234000 loss: 1.9549709558486938\n",
      "Iteration 234500 loss: 2.2818217277526855\n",
      "Iteration 235000 loss: 2.2025158405303955\n",
      "Iteration 235500 loss: 2.0238215923309326\n",
      "Iteration 236000 loss: 2.2033305168151855\n",
      "Iteration 236500 loss: 2.00718092918396\n",
      "Iteration 237000 loss: 2.1323297023773193\n",
      "Iteration 237500 loss: 2.117457628250122\n",
      "Iteration 238000 loss: 2.07936429977417\n",
      "Iteration 238500 loss: 2.0124318599700928\n",
      "Iteration 239000 loss: 2.0684726238250732\n",
      "Iteration 239500 loss: 2.2340400218963623\n",
      "Iteration 240000 loss: 1.9952150583267212\n",
      "Iteration 240500 loss: 2.254016399383545\n",
      "Iteration 241000 loss: 2.218045234680176\n",
      "Iteration 241500 loss: 2.106915235519409\n",
      "Iteration 242000 loss: 1.946947693824768\n",
      "Iteration 242500 loss: 2.23935604095459\n",
      "Iteration 243000 loss: 1.945277214050293\n",
      "Iteration 243500 loss: 1.8773317337036133\n",
      "Iteration 244000 loss: 2.3324222564697266\n",
      "Iteration 244500 loss: 1.9969173669815063\n",
      "Iteration 245000 loss: 2.0832338333129883\n",
      "Iteration 245500 loss: 2.3095452785491943\n",
      "Iteration 246000 loss: 2.1340291500091553\n",
      "Iteration 246500 loss: 2.2325127124786377\n",
      "Iteration 247000 loss: 1.958138108253479\n",
      "Iteration 247500 loss: 1.9149184226989746\n",
      "Iteration 248000 loss: 2.2298409938812256\n",
      "Iteration 248500 loss: 2.008747100830078\n",
      "Iteration 249000 loss: 2.0101611614227295\n",
      "Iteration 249500 loss: 2.105668544769287\n",
      "Iteration 250000 loss: 2.0828287601470947\n",
      "Iteration 250500 loss: 2.094987154006958\n",
      "Iteration 251000 loss: 2.158484935760498\n",
      "Iteration 251500 loss: 2.0500545501708984\n",
      "Iteration 252000 loss: 2.0993545055389404\n",
      "Iteration 252500 loss: 2.0925066471099854\n",
      "Iteration 253000 loss: 2.005927562713623\n",
      "Iteration 253500 loss: 2.0572121143341064\n",
      "Iteration 254000 loss: 2.071420907974243\n",
      "Iteration 254500 loss: 2.045822858810425\n",
      "Iteration 255000 loss: 2.1560254096984863\n",
      "Iteration 255500 loss: 1.9505770206451416\n",
      "Iteration 256000 loss: 2.1092634201049805\n",
      "Iteration 256500 loss: 2.225377082824707\n",
      "Iteration 257000 loss: 1.955283284187317\n",
      "Iteration 257500 loss: 2.11645245552063\n",
      "Iteration 258000 loss: 1.983045220375061\n",
      "Iteration 258500 loss: 2.189059019088745\n",
      "Iteration 259000 loss: 2.316774606704712\n",
      "Iteration 259500 loss: 2.080925226211548\n",
      "Iteration 260000 loss: 2.1383979320526123\n",
      "Iteration 260500 loss: 2.127979278564453\n",
      "Iteration 261000 loss: 2.138810396194458\n",
      "Iteration 261500 loss: 2.381382703781128\n",
      "Iteration 262000 loss: 2.1919572353363037\n",
      "Iteration 262500 loss: 2.068915605545044\n",
      "Iteration 263000 loss: 2.237429618835449\n",
      "Iteration 263500 loss: 2.044379949569702\n",
      "Iteration 264000 loss: 2.0681607723236084\n",
      "Iteration 264500 loss: 2.1345667839050293\n",
      "Iteration 265000 loss: 2.221015214920044\n",
      "Iteration 265500 loss: 1.8638067245483398\n",
      "Iteration 266000 loss: 2.2361299991607666\n",
      "Iteration 266500 loss: 2.2363882064819336\n",
      "Iteration 267000 loss: 2.126781940460205\n",
      "Iteration 267500 loss: 2.0764029026031494\n",
      "Iteration 268000 loss: 2.293544292449951\n",
      "Iteration 268500 loss: 2.23130202293396\n",
      "Iteration 269000 loss: 2.2581663131713867\n",
      "Iteration 269500 loss: 1.9594768285751343\n",
      "Iteration 270000 loss: 2.0362095832824707\n",
      "Iteration 270500 loss: 2.036972999572754\n",
      "Iteration 271000 loss: 2.1108245849609375\n",
      "Iteration 271500 loss: 2.1947686672210693\n",
      "Iteration 272000 loss: 2.1225054264068604\n",
      "Iteration 272500 loss: 1.9898922443389893\n",
      "Iteration 273000 loss: 2.062192440032959\n",
      "Iteration 273500 loss: 2.2322840690612793\n",
      "Iteration 274000 loss: 1.9920436143875122\n",
      "Iteration 274500 loss: 2.2274208068847656\n",
      "Iteration 275000 loss: 2.1775999069213867\n",
      "Iteration 275500 loss: 2.031067371368408\n",
      "Iteration 276000 loss: 2.12015438079834\n",
      "Iteration 276500 loss: 2.060481548309326\n",
      "Iteration 277000 loss: 1.9058129787445068\n",
      "Iteration 277500 loss: 2.0971429347991943\n",
      "Iteration 278000 loss: 2.1302614212036133\n",
      "Iteration 278500 loss: 2.1194775104522705\n",
      "Iteration 279000 loss: 2.074612617492676\n",
      "Iteration 279500 loss: 2.1038260459899902\n",
      "Iteration 280000 loss: 2.1954355239868164\n",
      "Iteration 280500 loss: 2.129286289215088\n",
      "Iteration 281000 loss: 2.067018508911133\n",
      "Iteration 281500 loss: 2.3365700244903564\n",
      "Iteration 282000 loss: 1.9450198411941528\n",
      "Iteration 282500 loss: 2.115884780883789\n",
      "Iteration 283000 loss: 2.1476359367370605\n",
      "Iteration 283500 loss: 2.2141318321228027\n",
      "Iteration 284000 loss: 1.9536504745483398\n",
      "Iteration 284500 loss: 1.9415574073791504\n",
      "Iteration 285000 loss: 1.9512373208999634\n",
      "Iteration 285500 loss: 2.138531446456909\n",
      "Iteration 286000 loss: 2.1958608627319336\n",
      "Iteration 286500 loss: 2.2554116249084473\n",
      "Iteration 287000 loss: 2.268617868423462\n",
      "Iteration 287500 loss: 1.9707450866699219\n",
      "Iteration 288000 loss: 2.08307147026062\n",
      "Iteration 288500 loss: 2.00809383392334\n",
      "Iteration 289000 loss: 2.0656306743621826\n",
      "Iteration 289500 loss: 1.8638277053833008\n",
      "Iteration 290000 loss: 2.030254602432251\n",
      "Iteration 290500 loss: 2.08402156829834\n",
      "Iteration 291000 loss: 1.9659465551376343\n",
      "Iteration 291500 loss: 2.039182662963867\n",
      "Iteration 292000 loss: 2.11439847946167\n",
      "Iteration 292500 loss: 2.357861042022705\n",
      "Iteration 293000 loss: 1.8823833465576172\n",
      "Iteration 293500 loss: 2.1960880756378174\n",
      "Iteration 294000 loss: 2.0951099395751953\n",
      "Iteration 294500 loss: 2.218456745147705\n",
      "Iteration 295000 loss: 2.159963846206665\n",
      "Iteration 295500 loss: 1.8492419719696045\n",
      "Iteration 296000 loss: 2.0807552337646484\n",
      "Iteration 296500 loss: 2.056274890899658\n",
      "Iteration 297000 loss: 1.9612882137298584\n",
      "Iteration 297500 loss: 2.1335928440093994\n",
      "Iteration 298000 loss: 2.12247633934021\n",
      "Iteration 298500 loss: 2.068012237548828\n",
      "Iteration 299000 loss: 2.143763303756714\n",
      "Iteration 299500 loss: 2.048414468765259\n"
     ]
    }
   ],
   "source": [
    "emb_size = 10\n",
    "block_size = 3\n",
    "num_chars = len(chars)\n",
    "hidden_layer_size = 200\n",
    "X, Y = build_inputs_and_labels(words, block_size)\n",
    "parameters = init_parameters(num_chars, block_size, emb_size, hidden_layer_size)\n",
    "losses = gradient_descent(X_train, Y_train, parameters, iterations=300000, minibatch_size=128, lr=0.2, lr_decay=0.00002, regularization=0.01, print_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x374e181f0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(losses).log10())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the train and validation losses are both improvements over our results in lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, y, parameters):\n",
    "    logits = forward(x, parameters)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.111666679382324"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_train, Y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1701574325561523"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_dev, Y_dev, parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we try the data on the test set, we acheive a similar result to validation. (If we hadn't we may have accidentally overfit to the validation set via tuning hyperparameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.168839693069458"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_test, Y_test, parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate some names with the trained model to verify the model works well:\n",
    "\n",
    "I added some features to the generate function that allows the user to specify a minimum and maximum length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(parameters, block_size, min_chars=3, max_chars=20):\n",
    "    C, W1, b1, W2, b2 = parameters\n",
    "    context = [0] * block_size\n",
    "    out = []\n",
    "    for i in range(max_chars):\n",
    "        logits = forward(torch.tensor([context]), parameters)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        if i < min_chars:\n",
    "            probs[0, 0] = 0\n",
    "        ix = torch.multinomial(probs, 1).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novann\n",
      "amarksan\n",
      "sten\n",
      "ishar\n",
      "jaycen\n",
      "kiplon\n",
      "angie\n",
      "giyton\n",
      "maritton\n",
      "zajay\n",
      "jzo\n",
      "zavan\n",
      "enio\n",
      "carah\n",
      "nan\n",
      "audy\n",
      "maebriyah\n",
      "nia\n",
      "man\n",
      "joshal\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generate(parameters, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fun, we can try to generate some really long names:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiurallinnylenn\n",
      "urmanielayus\n",
      "marionnaheal\n",
      "jaciuluanalind\n",
      "nafedencerem\n",
      "kayannanellissaitana\n",
      "praclynnedhana\n",
      "aknilanonilah\n",
      "aljahmanielle\n",
      "harlettonnah\n",
      "colostlinahna\n",
      "aadhitonterryx\n",
      "yairandreidia\n",
      "caylandrarsen\n",
      "coriannahica\n",
      "abdalyndoneh\n",
      "maxandrousuf\n",
      "baseontavieren\n",
      "breliianellah\n",
      "abdulvynceie\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generate(parameters, 3, min_chars=12, max_chars=40))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E02: \n",
    "#### *I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the predicted probabilities were perfectly uniform, per the log-likelihood formula the loss would be -log(1/27), given a dataset with 27 characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_actual = Y_train[0]\n",
    "y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.ones(27).divide(27)\n",
    "logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also apply a softmax activation, but it results in the exact same logits if the logits are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(logits, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it really doesn't matter what value the weights hold. However, the tanh activation is more informative when values are close to zero, so we should strive to keep the weights closer to zero (rather than say, initialize them all to 10000000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy(logits, y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.295836866004329"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "-math.log(1/27)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our starting loss is much higher (~23!). To acheive a similar loss, we could make every weight and bias uniform across layers throughout the network. Before, we sampled weights between 0 and 1. To acheive a starting loss of -log(1/27), we can initialize them all to the same value, 0.5 sounds appropriate given that it's in the center of our previous distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(num_chars, block_size, embedding_size, hidden_layer_size):\n",
    "    C = torch.full((num_chars, embedding_size), 0.5, requires_grad=True)\n",
    "    W1 = torch.full((embedding_size * block_size, hidden_layer_size), 0.5, requires_grad=True)\n",
    "    b1 = torch.full((hidden_layer_size,), 0.5, requires_grad=True)\n",
    "    W2 = torch.full((hidden_layer_size, num_chars), 0.5, requires_grad=True)\n",
    "    b2 = torch.full((num_chars,), 0.5, requires_grad=True)\n",
    "\n",
    "    return [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parameters = init_parameters(num_chars, block_size, emb_size, hidden_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_parameters[0][:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify that we acheive the desired starting loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.295837163925171"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_train, Y_train, new_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E03: \n",
    "#### *Read the Bengio et al 2003 paper (link above), implement and try any idea from the paper. Did it work?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One idea we could try from the paper is mixing n-gram (in this case, trigram) probabilities with the neural network probabilities to get a final probability, s.t. with each probability having a weight of 0.5. In their results, the network consistently performed better with this technique. The authors suggested that this suggests that the trigram and neural network models tend to make errors in different places.\n",
    "\n",
    "To acheive this, we can first instantiate and populate a trigram matrix. It's probably prudent just to compute this on the training dataset rather than the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = torch.zeros((27 * 27), 27)\n",
    "\n",
    "for word in train_words:\n",
    "    ids = ([0] * (block_size - 1)) + [stoi[c] for c in word] + [0]\n",
    "    for i in range(len(ids) - 2):\n",
    "        trigrams[(ids[i] * 27) + ids[i + 1], ids[i + 2]] += 1\n",
    "\n",
    "trigrams = (trigrams / trigrams.sum(dim=1, keepdim=True)).nan_to_num(nan=0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can update the evaluation function to average the network output and the trigram output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x, y, parameters, trigrams):\n",
    "    logits = forward(x, parameters) \n",
    "    preds = (torch.softmax(logits, dim=1) * 0.5) + (torch.softmax(trigrams[x[:, 1] * 27 + x[:, 2]], dim=1) * 0.5)\n",
    "    loss = -preds[torch.arange(y.shape[0]), y].log().mean()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.363494396209717"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_train, Y_train, parameters, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3897221088409424"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(X_dev, Y_dev, parameters, trigrams)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique does not improve our model's performance but does help the model generalize a bit better, as the train and dev losses are much closer. Perhaps this technique tends to work better on word-level models like in the paper rather than character-level models -- perhaps because the embeddings needed to properly represent a letter are much simpler than those for a word, meaning that ensambling our neural network with the trigram model does not compensate for any of the neural network's error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also try introducing the trigram mixing into our training loop, but I am skeptical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_step(x, y, parameters, trigrams, lr=0.1):\n",
    "    logits = forward(x, parameters)\n",
    "    preds = (torch.softmax(logits, dim=1) * 0.5) + (torch.softmax(trigrams[x[:, 1] * 27 + x[:, 2]], dim=1) * 0.5)\n",
    "    loss = -preds[torch.arange(y.shape[0]), y].log().mean()\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data -= p.grad * 0.1\n",
    "        p.grad = None\n",
    "    return loss.item()\n",
    "\n",
    "def gradient_descent(x, y, parameters, trigrams, iterations=100, minibatch_size=100, lr=0.1, lr_decay=0.001, regularization=0.01, print_every=10):\n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        batch_indices = torch.randint(0, x.shape[0], (minibatch_size,))\n",
    "        xi = x[batch_indices]\n",
    "        yi = y[batch_indices]\n",
    "        reg_term = torch.sum(torch.stack([(p**2).sum() for p in parameters])).divide(torch.sum(torch.tensor([p.numel() for p in parameters]))) * regularization\n",
    "        loss = optimize_step(xi, yi, parameters, trigrams, lr) + reg_term\n",
    "        losses.append(loss)\n",
    "        if i % print_every == 0:\n",
    "            print(f'Iteration {i} loss: {loss}')\n",
    "        lr = lr * (1 - lr_decay)\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 3.2184016704559326\n",
      "Iteration 500 loss: 2.877875328063965\n",
      "Iteration 1000 loss: 2.956172466278076\n",
      "Iteration 1500 loss: 2.78188157081604\n",
      "Iteration 2000 loss: 2.8619027137756348\n",
      "Iteration 2500 loss: 2.9384779930114746\n",
      "Iteration 3000 loss: 2.9826409816741943\n",
      "Iteration 3500 loss: 2.864387273788452\n",
      "Iteration 4000 loss: 3.1087021827697754\n",
      "Iteration 4500 loss: 2.7980666160583496\n",
      "Iteration 5000 loss: 2.8054986000061035\n",
      "Iteration 5500 loss: 2.9512250423431396\n",
      "Iteration 6000 loss: 2.847517251968384\n",
      "Iteration 6500 loss: 2.872109889984131\n",
      "Iteration 7000 loss: 3.022531747817993\n",
      "Iteration 7500 loss: 2.933150291442871\n",
      "Iteration 8000 loss: 2.857933282852173\n",
      "Iteration 8500 loss: 2.795549154281616\n",
      "Iteration 9000 loss: 2.9603750705718994\n",
      "Iteration 9500 loss: 2.9267566204071045\n",
      "Iteration 10000 loss: 2.8576321601867676\n",
      "Iteration 10500 loss: 2.9645564556121826\n",
      "Iteration 11000 loss: 2.9305906295776367\n",
      "Iteration 11500 loss: 2.934227228164673\n",
      "Iteration 12000 loss: 2.982835292816162\n",
      "Iteration 12500 loss: 2.9476053714752197\n",
      "Iteration 13000 loss: 2.994748830795288\n",
      "Iteration 13500 loss: 2.9265904426574707\n",
      "Iteration 14000 loss: 2.8131308555603027\n",
      "Iteration 14500 loss: 2.89304518699646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_parameters \u001b[39m=\u001b[39m init_parameters(num_chars, block_size, emb_size, hidden_layer_size)\n\u001b[0;32m----> 2\u001b[0m losses \u001b[39m=\u001b[39m gradient_descent(X_train, Y_train, new_parameters, trigrams, iterations\u001b[39m=\u001b[39;49m\u001b[39m300000\u001b[39;49m, minibatch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, lr_decay\u001b[39m=\u001b[39;49m\u001b[39m0.00002\u001b[39;49m, regularization\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, print_every\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[48], line 18\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(x, y, parameters, trigrams, iterations, minibatch_size, lr, lr_decay, regularization, print_every)\u001b[0m\n\u001b[1;32m     16\u001b[0m yi \u001b[39m=\u001b[39m y[batch_indices]\n\u001b[1;32m     17\u001b[0m reg_term \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack([(p\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msum() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters]))\u001b[39m.\u001b[39mdivide(torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mtensor([p\u001b[39m.\u001b[39mnumel() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters]))) \u001b[39m*\u001b[39m regularization\n\u001b[0;32m---> 18\u001b[0m loss \u001b[39m=\u001b[39m optimize_step(xi, yi, parameters, trigrams, lr) \u001b[39m+\u001b[39m reg_term\n\u001b[1;32m     19\u001b[0m losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m, in \u001b[0;36moptimize_step\u001b[0;34m(x, y, parameters, trigrams, lr)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize_step\u001b[39m(x, y, parameters, trigrams, lr\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     logits \u001b[39m=\u001b[39m forward(x, parameters)\n\u001b[1;32m      3\u001b[0m     preds \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m) \u001b[39m+\u001b[39m (torch\u001b[39m.\u001b[39msoftmax(trigrams[x[:, \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m27\u001b[39m \u001b[39m+\u001b[39m x[:, \u001b[39m2\u001b[39m]], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n\u001b[1;32m      4\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpreds[torch\u001b[39m.\u001b[39marange(y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), y]\u001b[39m.\u001b[39mlog()\u001b[39m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(x, parameters)\u001b[0m\n\u001b[1;32m      3\u001b[0m emb \u001b[39m=\u001b[39m C[x]\n\u001b[1;32m      4\u001b[0m h \u001b[39m=\u001b[39m (emb\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, emb\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m emb\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]) \u001b[39m@\u001b[39m W1) \u001b[39m+\u001b[39m b1\n\u001b[0;32m----> 5\u001b[0m h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49mtanh()\n\u001b[1;32m      6\u001b[0m h \u001b[39m=\u001b[39m (h \u001b[39m@\u001b[39m W2) \u001b[39m+\u001b[39m b2\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m h\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_parameters = init_parameters(num_chars, block_size, emb_size, hidden_layer_size)\n",
    "losses = gradient_descent(X_train, Y_train, new_parameters, trigrams, iterations=300000, minibatch_size=128, lr=0.2, lr_decay=0.00002, regularization=0.01, print_every=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After many iterations with no signs of life, I decided to stop training early."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

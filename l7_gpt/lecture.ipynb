{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-13 14:49:07--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.10’\n",
      "\n",
      "input.txt.10        100%[===================>]   1.06M  4.94MB/s    in 0.2s    \n",
      "\n",
      "2023-05-13 14:49:07 (4.94 MB/s) - ‘input.txt.10’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Now that we have an encoder, we can encode the entire dataset:\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can split our dataset into training and validation datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spl = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:spl]\n",
    "\n",
    "val_data = data[spl:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data will contain mini-batches of blocks on tokens with block size <= block_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "def get_batch(split, batch_size, context_length):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train', batch_size, context_length)\n",
    "\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_length):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context.tolist()} the target: {target}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our input/label generation sorted, we can start training a model. We can start with the BigramLanguageModel we've implemented in previous lectures as a baseline. We simply look up a probabilitiy distribution in the token embedding table and use that to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_steps, batch_size, context_length, learning_rate=1e-3, optimizer=None, print_every=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for step in range(1, num_steps+1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = get_batch('train', batch_size, context_length)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % print_every == 0:\n",
    "            print(f'step {step}: loss {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def plot_ema(losses, gamma=0.99, title='ema'):\n",
    "    ema = losses[0]\n",
    "    ema_losses = []\n",
    "    for i, l in enumerate(losses):\n",
    "        ema = gamma * ema + (1-gamma) * l\n",
    "        ema_losses.append(ema)\n",
    "    plt.plot(ema_losses, label='loss ema', color='red')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('final loss (ema):', ema_losses[-1])\n",
    "\n",
    "def generate_text(model, starting_text=' ', max_new_tokens=100):\n",
    "    data = torch.tensor(encode(starting_text), dtype=torch.long, device=device).reshape(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(starting_text + decode(model.generate(data, max_new_tokens=max_new_tokens)[0].tolist()))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1,1), dtype=torch.long, device=device).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLangugageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # batch, timestep, channel\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLangugageModel(vocab_size).to(device)\n",
    "out, loss = m(xb, yb)\n",
    "print('loss:', loss)\n",
    "print(out.shape)\n",
    "generate_text(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lectures we've implemented our own simple SGD optimizer -- this time we can just use one of PyTorch's built-in optimizers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the optimizer to train the Bigram model over a few steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss 3.679224729537964\n",
      "step 2000: loss 3.0610170364379883\n",
      "step 3000: loss 2.72971773147583\n",
      "step 4000: loss 2.545755386352539\n",
      "step 5000: loss 2.509843349456787\n",
      "step 6000: loss 2.4547369480133057\n",
      "step 7000: loss 2.4333791732788086\n",
      "step 8000: loss 2.4855430126190186\n",
      "step 9000: loss 2.4706852436065674\n",
      "step 10000: loss 2.466132879257202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss (ema): 2.460625844800048\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "context_length = 8\n",
    "losses = train(m, num_steps=10000, context_length=32, batch_size=32)\n",
    "plot_ema(losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to bring loss down from its initial value, but the model fails to improve much past ~2.5.\n",
    "\n",
    "We can also try generating some tokens with our model, as you can see its still mostly giberish, but is closer to shakespearian engish than truly random characters would be (there are a few recognizable words, and words take on typical word lengths with punctuations interspersed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "  arllendou te ang, INULAnes.\n",
      "RY:\n",
      "\n",
      "Henpe, d n!\n",
      "K:\n",
      "APO t'theronem' more isheppe MIRGI ane poug winond, \n"
     ]
    }
   ],
   "source": [
    "generate_text(m)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can implement a slightly more sophisticated model:\n",
    "* we initialize a token and positional embedding table -- in addition to building embeddings for each token we also create embeddings for each relative position \n",
    "* we can then combine these embeddings into a single embedding using addition\n",
    "* we can also specify the size of these embeddings, and then use a linear layer to downsample back to the desired output size (vocab size)\n",
    "\n",
    "\n",
    "Note: Andrej really likes using global variables everywhere -- this gets too crazy for my liking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoolerBigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, context_length) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embedding_table = nn.Embedding(context_length, emb_size)\n",
    "        self.output_layer = nn.Linear(emb_size, vocab_size)\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # get token embeddings from input indices\n",
    "        token_embeddings = self.token_embedding_table(idx)\n",
    "\n",
    "        # get positional embeddings for each timestep\n",
    "        B, T, C = token_embeddings.shape\n",
    "        positional_embeddings = self.positional_embedding_table(torch.arange(T, device=idx.device))\n",
    "\n",
    "        # add them together\n",
    "        embeddings = token_embeddings + positional_embeddings   \n",
    "\n",
    "        # pass thru linear layer\n",
    "        logits = self.output_layer(embeddings)\n",
    "\n",
    "        # everything else is the same as before\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # same as before\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0485, -0.3754,  0.1733,  ..., -0.5229,  1.4596, -0.7508],\n",
       "         [ 1.3015, -0.8666, -1.4632,  ...,  0.3292,  0.6797,  0.6603],\n",
       "         [ 0.1064, -0.2407,  0.3348,  ..., -0.0533, -0.7951,  0.0379],\n",
       "         ...,\n",
       "         [ 0.9822, -0.1071,  0.4448,  ..., -0.3036,  0.4441, -1.0098],\n",
       "         [ 0.0878,  1.0063, -0.3769,  ..., -0.2763,  1.4296, -0.5280],\n",
       "         [-0.3246, -0.5319,  0.5530,  ...,  1.3165, -0.2661,  0.6586]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor(4.4531, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 8\n",
    "batch_size = 32\n",
    "emb_size = 32\n",
    "cooler_model = CoolerBigramLanguageModel(vocab_size, emb_size=emb_size, context_length=context_length).to(device)\n",
    "cooler_model(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss 2.5511012077331543\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[39m=\u001b[39m train(cooler_model, \u001b[39m10000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, context_length\u001b[39m=\u001b[39;49mcontext_length)\n\u001b[1;32m      2\u001b[0m plot_ema(losses)\n\u001b[1;32m      3\u001b[0m generate_text(cooler_model)\n",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m      8\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[1;32m      9\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[0;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = train(cooler_model, 10000, batch_size=batch_size, context_length=context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(cooler_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can implement a self-attention head, then create a model that propogates through a self-attention head prior to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length) -> None:\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape\n",
    "        \n",
    "        # lookup query, key, and value vectors\n",
    "        # C == input_channels, H == output_channels == head_size\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, H)\n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, H)\n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, H)\n",
    "\n",
    "        # compute self attention by taking dot product of query and key\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "\n",
    "        wei *= self.output_channels ** -0.5 # scale by sqrt of head size\n",
    "        \n",
    "        # apply lower triangular mask to weights\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        # apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # apply attention weights to values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "\n",
    "        return out # (B, T, H)\n",
    "    \n",
    "    \n",
    "class SingleHeadModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.attention_head = SelfAttentionHead(emb_size, head_size, context_length)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.attention_head(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      4\u001b[0m single_head_model \u001b[39m=\u001b[39m SingleHeadModel(vocab_size, emb_size\u001b[39m=\u001b[39memb_size, head_size\u001b[39m=\u001b[39memb_size, context_length\u001b[39m=\u001b[39mcontext_length)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(single_head_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(single_head_model)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[0;32m----> 9\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 66\u001b[0m, in \u001b[0;36mSingleHeadModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     64\u001b[0m     logits \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mview(B\u001b[39m*\u001b[39mT, V)\n\u001b[1;32m     65\u001b[0m     targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mview(B\u001b[39m*\u001b[39mT)\n\u001b[0;32m---> 66\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(logits, targets)\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_length = 32\n",
    "batch_size = 32\n",
    "emb_size = 32\n",
    "single_head_model = SingleHeadModel(vocab_size, emb_size=emb_size, head_size=emb_size, context_length=context_length).to(device)\n",
    "losses = train(single_head_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(single_head_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, instead of using a single attention head, we can try implementing Multi-Head Attention, which concatenates the results of multiple attention heads running in parallel.\n",
    "\n",
    "The below implementation is not really parallel, but this is a task left for the exercises (I implement it in parallel there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, context_length, head_size, num_heads) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(emb_size, head_size, context_length) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "\n",
    "class MultiHeadModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.attention_heads = MultiHeadAttention(emb_size, context_length, head_size//num_multi_attn_heads, num_heads=num_multi_attn_heads)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.attention_heads(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_head_model \u001b[39m=\u001b[39m MultiHeadModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_head_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_head_model)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[0;32m----> 9\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[45], line 33\u001b[0m, in \u001b[0;36mMultiHeadModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     30\u001b[0m x \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m p \u001b[39m# (B, T, C) (broadcasting)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# pass thru attention head\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_heads(x) \u001b[39m# (B, T, C) -> (B, T, H)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# pass thru output layer\u001b[39;00m\n\u001b[1;32m     36\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(x) \u001b[39m# (B, T, H) -> (B, T, V)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 29\u001b[0m, in \u001b[0;36mSelfAttentionHead.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m wei \u001b[39m=\u001b[39m wei\u001b[39m.\u001b[39mmasked_fill(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtril[:T,:T]\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# apply softmax to get attention weights\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m wei \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49msoftmax(wei, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# apply attention weights to values\u001b[39;00m\n\u001b[1;32m     32\u001b[0m out \u001b[39m=\u001b[39m wei \u001b[39m@\u001b[39m v \u001b[39m# (B, T, T) @ (B, T, H) -> (B, T, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/functional.py:1834\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1833\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1834\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[1;32m   1835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1836\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_length = 32\n",
    "batch_size = 32\n",
    "emb_size = 32\n",
    "multi_head_model = MultiHeadModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=4).to(device)\n",
    "losses = train(multi_head_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(multi_head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "    \n",
    "\n",
    "class MultiHeadwFFModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads,) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.attention_heads = MultiHeadAttention(emb_size, context_length, head_size//num_multi_attn_heads, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.attention_heads(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_head_wff_model \u001b[39m=\u001b[39m MultiHeadwFFModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_head_wff_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_head_wff_model, \u001b[39m'\u001b[39m\u001b[39mAlex Shadley:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m      9\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m: loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    162\u001b[0m           grads,\n\u001b[1;32m    163\u001b[0m           exp_avgs,\n\u001b[1;32m    164\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    165\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           state_steps,\n\u001b[1;32m    167\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    168\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    169\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    170\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m func(params,\n\u001b[1;32m    219\u001b[0m      grads,\n\u001b[1;32m    220\u001b[0m      exp_avgs,\n\u001b[1;32m    221\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    222\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      state_steps,\n\u001b[1;32m    224\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    225\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    226\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    229\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    230\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    231\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/optim/adamw.py:309\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    307\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    311\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_length = 32\n",
    "batch_size = 32\n",
    "emb_size = 32\n",
    "multi_head_wff_model = MultiHeadwFFModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=4).to(device)\n",
    "losses = train(multi_head_wff_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(multi_head_wff_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, context_length, head_size//num_multi_attn_heads, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_block_model \u001b[39m=\u001b[39m MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, num_blocks\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_block_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_block_model)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[0;32m----> 9\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 36\u001b[0m, in \u001b[0;36mMultiBlockModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m p \u001b[39m# (B, T, C) (broadcasting)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# pass thru attention head\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B, T, C) -> (B, T, H)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# pass thru feed forward\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(x) \u001b[39m# (B, T, H) -> (B, T, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m, in \u001b[0;36mResidualTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)) \u001b[39m+\u001b[39;49m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "context_length = 32\n",
    "batch_size = 32\n",
    "emb_size = 32\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=4, num_blocks=4).to(device)\n",
    "losses = train(multi_block_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(multi_block_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can try scaling up some of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_block_model \u001b[39m=\u001b[39m MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_blocks\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_block_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_block_model)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[0;32m----> 9\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 36\u001b[0m, in \u001b[0;36mMultiBlockModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m p \u001b[39m# (B, T, C) (broadcasting)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# pass thru attention head\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B, T, C) -> (B, T, H)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# pass thru feed forward\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(x) \u001b[39m# (B, T, H) -> (B, T, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m, in \u001b[0;36mResidualTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[33], line 26\u001b[0m, in \u001b[0;36mSelfAttentionHead.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m wei \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_channels \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m# scale by sqrt of head size\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# apply lower triangular mask to weights\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m wei \u001b[39m=\u001b[39m wei\u001b[39m.\u001b[39;49mmasked_fill(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtril[:T,:T]\u001b[39m==\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39mfloat\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m-inf\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# apply softmax to get attention weights\u001b[39;00m\n\u001b[1;32m     29\u001b[0m wei \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(wei, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B, T, T)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8).to(device)\n",
    "losses = train(multi_block_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(multi_block_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

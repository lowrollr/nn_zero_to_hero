{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture we implemented multi-head attention by feeding data into several different 'Head' modules in series and then concating the results. We can implement a vectorized version by combining the linear layers of each head into unified linear layers. We just need to be sure to get the dimensions correctly matched up, so that when we create a dimension for each head prior to computing the query-key dot product. I've outlined the shapes of each intermediate tensor in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length) -> None:\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape\n",
    "        \n",
    "        # lookup query, key, and value vectors\n",
    "        # C == input_channels, H == output_channels == head_size\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, H)\n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, H)\n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, H)\n",
    "\n",
    "        # compute self attention by taking dot product of query and key\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "\n",
    "        wei *= self.output_channels ** -0.5 # scale by sqrt of head size\n",
    "        \n",
    "        # apply lower triangular mask to weights\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        # apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # apply attention weights to values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "\n",
    "        return out # (B, T, H)\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, context_length, head_size, num_heads) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(emb_size, head_size, context_length) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, context_length, head_size//num_multi_attn_heads, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length, num_heads) -> None:\n",
    "        assert output_channels % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.head_size = output_channels // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape \n",
    "        # B = batches\n",
    "        # T = timesteps\n",
    "        # C = input channels\n",
    "        # Nh = attention heads\n",
    "        # Hs = head size\n",
    "        \n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        \n",
    "        q = q.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "        k = k.transpose(-2, -1).view(B, self.num_heads, self.head_size, T) # (B, T, Nh * Hs) -> (B, Nh, Hs, T)\n",
    "        \n",
    "        w = q @ k # (B, Nh, T, Hs) @ (B, Nh, Hs, T) -> (B, Nh, T, T)\n",
    "        w = w.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "\n",
    "        w *= self.head_size**-0.5\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        \n",
    "        \n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        v = v.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "\n",
    "        # torch requires a reshape here for reasons I don't fully understand\n",
    "        attn_out = (w @ v).transpose(-3, -2).reshape(B, T, -1)\n",
    "\n",
    "        return attn_out\n",
    "    \n",
    "\n",
    "# Need to redifine classes that use MultiHeadAttention as well!\n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, head_size, context_length, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try training a model using this new component. I'm going to import some other code that I won't explain here from the 'lilgpt' module I built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "spl = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:spl]\n",
    "\n",
    "val_data = data[spl:]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "def get_batch(split, batch_size, context_length):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "def train(model, num_steps, batch_size, context_length, learning_rate=1e-3, optimizer=None, print_every=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for step in range(1, num_steps+1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = get_batch('train', batch_size, context_length)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % print_every == 0:\n",
    "            print(f'step {step}: loss {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def plot_ema(losses, gamma=0.99, title='ema'):\n",
    "    ema = losses[0]\n",
    "    ema_losses = []\n",
    "    for i, l in enumerate(losses):\n",
    "        ema = gamma * ema + (1-gamma) * l\n",
    "        ema_losses.append(ema)\n",
    "    plt.plot(ema_losses, label='loss ema', color='red')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('final loss (ema):', ema_losses[-1])\n",
    "\n",
    "def generate_text(model, starting_text=' ', max_new_tokens=100):\n",
    "    data = torch.tensor(encode(starting_text), dtype=torch.long, device=device).reshape(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(starting_text + decode(model.generate(data, max_new_tokens=max_new_tokens)[0].tolist()))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_block_model \u001b[39m=\u001b[39m MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_blocks\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_block_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_block_model)\n",
      "Cell \u001b[0;32mIn[20], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[0;32m---> 40\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 80\u001b[0m, in \u001b[0;36mMultiBlockModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m x \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m p \u001b[39m# (B, T, C) (broadcasting)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# pass thru attention head\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B, T, C) -> (B, T, H)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# pass thru feed forward\u001b[39;00m\n\u001b[1;32m     83\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(x) \u001b[39m# (B, T, H) -> (B, T, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mResidualTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 55\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_size, T)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B, T, Nh * Hs) -> (B, Nh, T, Hs)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# torch requires a reshape here for reasons I don't fully understand\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m attn_out \u001b[39m=\u001b[39m (w \u001b[39m@\u001b[39;49m v)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(B, T, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m attn_out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8).to(device)\n",
    "losses = train(multi_block_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses)\n",
    "generate_text(multi_block_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling in our code from lecture, we left off with the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length) -> None:\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape\n",
    "        \n",
    "        # lookup query, key, and value vectors\n",
    "        # C == input_channels, H == output_channels == head_size\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, H)\n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, H)\n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, H)\n",
    "\n",
    "        # compute self attention by taking dot product of query and key\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "\n",
    "        wei *= self.output_channels ** -0.5 # scale by sqrt of head size\n",
    "        \n",
    "        # apply lower triangular mask to weights\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        # apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # apply attention weights to values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "\n",
    "        return out # (B, T, H)\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, context_length, head_size, num_heads) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(emb_size, head_size, context_length) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, context_length, head_size//num_multi_attn_heads, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture we implemented multi-head attention by feeding data into several different 'Head' modules in series and then concating the results. We can implement a vectorized version by combining the linear layers of each head into unified linear layers. We just need to be sure to get the dimensions correctly matched up, so that when we create a dimension for each head prior to computing the query-key dot product. I've outlined the shapes of each intermediate tensor in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length, num_heads) -> None:\n",
    "        assert output_channels % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.head_size = output_channels // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape \n",
    "        # B = batches\n",
    "        # T = timesteps\n",
    "        # C = input channels\n",
    "        # Nh = attention heads\n",
    "        # Hs = head size\n",
    "        \n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        \n",
    "        q = q.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "        k = k.transpose(-2, -1).view(B, self.num_heads, self.head_size, T) # (B, T, Nh * Hs) -> (B, Nh, Hs, T)\n",
    "        \n",
    "        w = q @ k # (B, Nh, T, Hs) @ (B, Nh, Hs, T) -> (B, Nh, T, T)\n",
    "        w = w.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "\n",
    "        w *= self.head_size**-0.5\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        \n",
    "        \n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        v = v.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "\n",
    "        # torch requires a reshape here for reasons I don't fully understand\n",
    "        attn_out = (w @ v).transpose(-3, -2).reshape(B, T, -1)\n",
    "\n",
    "        return attn_out\n",
    "    \n",
    "\n",
    "# Need to redifine classes that use MultiHeadAttention as well!\n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, head_size, context_length, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try training a model using this new component. If done correctly we should receive nearly identical results when compared to those from the lecture notebook, as the underlying structure of the network should not be fundamentally different and none of the hyperparameters have otherwise changed. \n",
    "\n",
    "First I'll pull in the all the training utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "chars = sorted(list(set(string.printable)))\n",
    "vocab_size = len(chars) + 1\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "stoi[len(stoi)] = 'ukn'\n",
    "itos['ukn'] = len(itos)\n",
    "\n",
    "\n",
    "encode = lambda s: [stoi.get(c, itos['ukn']) for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "spl = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:spl]\n",
    "\n",
    "val_data = data[spl:]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "def get_batch(split, batch_size, context_length):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "def evaluate(model, batch_size, context_length, num_batches = 100):\n",
    "    model.eval()\n",
    "    x, y = get_batch('val', batch_size, context_length)\n",
    "    losses = []\n",
    "    for _ in range(num_batches):\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    loss = np.mean(losses)\n",
    "    model.train()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, num_steps, batch_size, context_length, learning_rate=1e-3, optimizer=None, print_every=1000, evaluate_every=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    eval_losses = []\n",
    "    for step in range(1, num_steps+1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = get_batch('train', batch_size, context_length)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % print_every == 0:\n",
    "            print(f'step {step}: loss {loss.item()}')\n",
    "\n",
    "        if step == 0 or step % evaluate_every == 0:\n",
    "            eval_loss = evaluate(model, batch_size, context_length)\n",
    "            eval_losses.append(eval_loss)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "\n",
    "    return losses, eval_losses\n",
    "\n",
    "def plot_ema(losses, eval_losses, gamma=0.99, title='ema'):\n",
    "    ema = losses[0]\n",
    "    ema_losses = []\n",
    "    eval_indices = np.linspace(0, len(losses), len(eval_losses), dtype=int)\n",
    "    for i, l in enumerate(losses):\n",
    "        ema = gamma * ema + (1-gamma) * l\n",
    "        ema_losses.append(ema)\n",
    "    plt.plot(ema_losses, label='train', color='red')\n",
    "    plt.plot(eval_indices, eval_losses, label='val', color='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('final train loss (ema):', ema_losses[-1])\n",
    "    print('final validation loss:', eval_losses[-1])\n",
    "\n",
    "def generate_text(model, starting_text=' ', max_new_tokens=100):\n",
    "    data = torch.tensor(encode(starting_text), dtype=torch.long, device=device).reshape(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(starting_text + decode(model.generate(data, max_new_tokens=max_new_tokens)[0].tolist()))\n",
    "    model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the new model with identical parameters to the last training run from lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_block_model \u001b[39m=\u001b[39m MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_blocks\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_block_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_block_model)\n",
      "Cell \u001b[0;32mIn[20], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[0;32m---> 40\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[1;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     42\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 80\u001b[0m, in \u001b[0;36mMultiBlockModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m x \u001b[39m=\u001b[39m t \u001b[39m+\u001b[39m p \u001b[39m# (B, T, C) (broadcasting)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m# pass thru attention head\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B, T, C) -> (B, T, H)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[39m# pass thru feed forward\u001b[39;00m\n\u001b[1;32m     83\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(x) \u001b[39m# (B, T, H) -> (B, T, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 55\u001b[0m, in \u001b[0;36mResidualTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 55\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)) \u001b[39m+\u001b[39m x \u001b[39m# residual\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_size, T)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B, T, Nh * Hs) -> (B, Nh, T, Hs)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# torch requires a reshape here for reasons I don't fully understand\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m attn_out \u001b[39m=\u001b[39m (w \u001b[39m@\u001b[39;49m v)\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(B, T, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m attn_out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8).to(device)\n",
    "losses, eval_losses = train(multi_block_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses, eval_losses)\n",
    "evaluate(multi_block_model, batch_size, context_length)\n",
    "generate_text(multi_block_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a loss of ~1.49, our results are almost exactly the same, which means our implementation should be correct. This new implementation computes the entire multi-head attention layer in a vectorized manner rather than concatenating results from "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/marshingjay/.cache/huggingface/datasets/ccdv___cnn_dailymail/1.0.0/1.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n",
      "100%|██████████| 3/3 [00:00<00:00, 130.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"ccdv/cnn_dailymail\", \"1.0.0\")\n",
    "train_data = data['train']\n",
    "val_data = data['validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m emb_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      4\u001b[0m multi_block_model \u001b[39m=\u001b[39m MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, num_blocks\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m losses \u001b[39m=\u001b[39m train(multi_block_model, \u001b[39m10000\u001b[39;49m, batch_size, context_length)\n\u001b[1;32m      6\u001b[0m plot_ema(losses)\n\u001b[1;32m      7\u001b[0m generate_text(multi_block_model)\n",
      "Cell \u001b[0;32mIn[86], line 37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_steps, batch_size, context_length, learning_rate, optimizer, print_every)\u001b[0m\n\u001b[1;32m     35\u001b[0m x, y \u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, batch_size, context_length)\n\u001b[1;32m     36\u001b[0m logits, loss \u001b[39m=\u001b[39m model(x, y)\n\u001b[0;32m---> 37\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/python3/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8).to(device)\n",
    "losses, eval_losses = train(multi_block_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses, eval_losses)\n",
    "generate_text(multi_block_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling in our code from lecture, we left off with the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length, dropout=0.05) -> None:\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape\n",
    "        \n",
    "        # lookup query, key, and value vectors\n",
    "        # C == input_channels, H == output_channels == head_size\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, H)\n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, H)\n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, H)\n",
    "\n",
    "        # compute self attention by taking dot product of query and key\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "\n",
    "        wei *= self.output_channels ** -0.5 # scale by sqrt of head size\n",
    "        \n",
    "        # apply lower triangular mask to weights\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        # apply softmax to get attention weights\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        wei = self.dropout(wei) # (B, T, T)\n",
    "\n",
    "        # apply attention weights to values\n",
    "        out = wei @ v # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "\n",
    "        return out # (B, T, H)\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(input_channels, output_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, context_length, head_size, num_heads) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(emb_size, head_size, context_length) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, context_length, head_size//num_multi_attn_heads, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture we implemented multi-head attention by feeding data into several different 'Head' modules in series and then concating the results. We can implement a vectorized version by combining the linear layers of each head into unified linear layers. We just need to be sure to get the dimensions correctly matched up, so that when we create a dimension for each head prior to computing the query-key dot product. I've outlined the shapes of each intermediate tensor in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length, num_heads, dropout=0.05) -> None:\n",
    "        assert output_channels % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.head_size = output_channels // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape \n",
    "        # B = batches\n",
    "        # T = timesteps\n",
    "        # C = input channels\n",
    "        # Nh = attention heads\n",
    "        # Hs = head size\n",
    "        \n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        \n",
    "        q = q.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "        k = k.transpose(-2, -1).view(B, self.num_heads, self.head_size, T) # (B, T, Nh * Hs) -> (B, Nh, Hs, T)\n",
    "        \n",
    "        w = q @ k # (B, Nh, T, Hs) @ (B, Nh, Hs, T) -> (B, Nh, T, T)\n",
    "        w = w.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "\n",
    "        w *= self.head_size**-0.5\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        w = self.dropout(w)\n",
    "        \n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        v = v.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "\n",
    "        # torch requires a reshape here for reasons I don't fully understand\n",
    "        attn_out = (w @ v).transpose(-3, -2).reshape(B, T, -1)\n",
    "\n",
    "        return attn_out\n",
    "    \n",
    "\n",
    "# Need to redifine classes that use MultiHeadAttention as well!\n",
    "\n",
    "class ResidualTransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(emb_size, head_size, context_length, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return x\n",
    "    \n",
    "class MultiBlockModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[ResidualTransformerBlock(emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try training a model using this new component. If done correctly we should receive nearly identical results when compared to those from the lecture notebook, as the underlying structure of the network should not be fundamentally different and none of the hyperparameters have otherwise changed. \n",
    "\n",
    "First I'll pull in the all the training utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "chars = sorted(list(set(string.printable)))\n",
    "vocab_size = len(chars) + 1\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "stoi['ukn'] = len(stoi)\n",
    "itos[len(itos)] = 'ukn'\n",
    "\n",
    "\n",
    "encode = lambda s: [stoi.get(c, stoi['ukn']) for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "spl = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:spl]\n",
    "\n",
    "val_data = data[spl:]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "context_length = 8\n",
    "\n",
    "def get_batch(split, batch_size, context_length):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "def evaluate(model, batch_size, context_length, num_batches = 100):\n",
    "    model.eval()\n",
    "    x, y = get_batch('val', batch_size, context_length)\n",
    "    losses = []\n",
    "    for _ in range(num_batches):\n",
    "        _, loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    loss = np.mean(losses)\n",
    "    model.train()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(model, num_steps, batch_size, context_length, learning_rate=1e-3, optimizer=None, print_every=1000, evaluate_every=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    eval_losses = []\n",
    "    for step in range(1, num_steps+1):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = get_batch('train', batch_size, context_length)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % print_every == 0:\n",
    "            print(f'step {step}: loss {loss.item()}')\n",
    "\n",
    "        if step == 0 or step % evaluate_every == 0:\n",
    "            eval_loss = evaluate(model, batch_size, context_length)\n",
    "            eval_losses.append(eval_loss)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "\n",
    "    return losses, eval_losses\n",
    "\n",
    "def plot_ema(losses, eval_losses, gamma=0.99, title='ema'):\n",
    "    ema = losses[0]\n",
    "    ema_losses = []\n",
    "    eval_indices = np.linspace(0, len(losses), len(eval_losses), dtype=int)\n",
    "    for i, l in enumerate(losses):\n",
    "        ema = gamma * ema + (1-gamma) * l\n",
    "        ema_losses.append(ema)\n",
    "    plt.plot(ema_losses, label='train', color='red')\n",
    "    plt.plot(eval_indices, eval_losses, label='val', color='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('final train loss (ema):', ema_losses[-1])\n",
    "    print('final validation loss:', eval_losses[-1])\n",
    "\n",
    "def generate_text(model, starting_text=None, max_new_tokens=100):\n",
    "    if starting_text is None:\n",
    "        starting_text = ''\n",
    "        data = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    else:\n",
    "        data = torch.tensor(encode(starting_text), dtype=torch.long, device=device).reshape(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(starting_text + decode(model.generate(data, max_new_tokens=max_new_tokens)[-1].tolist()))\n",
    "    model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the new model with identical parameters to the last training run from lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss 2.102978229522705\n",
      "step 2000: loss 1.8773711919784546\n",
      "step 3000: loss 1.755046010017395\n",
      "step 4000: loss 1.69569993019104\n",
      "step 5000: loss 1.686732292175293\n",
      "step 6000: loss 1.5767172574996948\n",
      "step 7000: loss 1.5750452280044556\n",
      "step 8000: loss 1.5459606647491455\n",
      "step 9000: loss 1.5760196447372437\n",
      "step 10000: loss 1.566866397857666\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train loss (ema): 1.5604387851971326\n",
      "final validation loss: 1.7794170379638672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7175747156143188"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8).to(device)\n",
    "losses, eval_losses = train(multi_block_model, 10000, batch_size, context_length)\n",
    "plot_ema(losses, eval_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a loss of ~1.56, our results are almost exactly the same as before, which means our implementation should be correct. This new implementation computes the entire multi-head attention layer in a vectorized manner rather than concatenating results from self-attention heads in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tir water's blower-with word of the you.\n",
      "\n",
      "Second Norby, so nece sware consulferooks\n",
      "And cappite joy, bable us made beserve eance\n",
      "Will me impts and fouth trift. Cowidon, all I care: I fie,\n",
      "To readge, me breath.\n",
      "\n",
      "KING HENRY VI:\n",
      "No, make were consinged in Maniaghtes:\n",
      "And a threforines head, sprop my mighty\n",
      "The rest all be hightly us' meath,\n",
      "And, Clebmothy and in his such from shapperlious\n",
      "A knight duty uc, hath he streng the conptient.\n",
      "\n",
      "Get thou of mine myseln greess? Well,\n",
      "The gree--Richbard, your afforth faced Lances\n",
      "Thou yech are mid exteranchs to made knock.\n",
      "And this cive dies did boy new thy say,\n",
      "All bews in this dight-prof times of recomanious?\n",
      "I know your art to his beitanating to their belle.\n",
      "O bromes him had cold perforges live,\n",
      "It my lest duty may, when that of vage head,\n",
      "'Ren boons: the that kingly throus harm father\n",
      "laight a he daughterting else keeep falle beeen\n",
      "and the has apinty chafe me appersh'd,\n",
      "Till these by son\n",
      "In must with him. Vinale and you, I sacions is:\n",
      "Thou this, \n"
     ]
    }
   ],
   "source": [
    "generate_text(multi_block_model, None, 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the huggingface datasets library to easily access and choose from amongst a large collection of nlp datasets. For this exercise, I've chosen to use the 'cnn_dailymail' dataset, which is comrpised of online news articles.\n",
    "\n",
    "This dataset is also already split into a train and validation set, which is nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Found cached dataset cnn_dailymail (/home/ubuntu/.cache/huggingface/datasets/ccdv___cnn_dailymail/1.0.0/1.0.0/0107f7388b5c6fae455a5661bcd134fc22da53ea75852027040d8d1e997f101f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdd32a5ce4d4113987ae64b6db2c6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"ccdv/cnn_dailymail\", \"1.0.0\")\n",
    "train_data = data['train']\n",
    "val_data = data['validation']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to write a new get_batch function, since it would be a bit cumbersome to load the entire dataset into a single tensor. Instead, we can access an article at random, and then pull out a random subsection of text from within that article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size, context_length):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix1 = torch.randint(len(data), (batch_size,))\n",
    "    \n",
    "    articles = [data[i.item()]['article'] for i in ix1]\n",
    "    encoded_articles = [encode(a) for a in articles]\n",
    "    tokenized_articles = torch.tensor([encoding for article in encoded_articles for encoding in article], dtype=torch.long, device=device)\n",
    "    ix2 = torch.randint(len(tokenized_articles) - context_length, (batch_size,))\n",
    "    x = torch.stack([tokenized_articles[i:i+context_length] for i in ix2])\n",
    "    y = torch.stack([tokenized_articles[i+1:i+context_length+1] for i in ix2])\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train the model just the same as before. Since the dataset is a bit larger we'll train for twice as many steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss 2.1318159103393555\n",
      "step 2000: loss 1.886654257774353\n",
      "step 3000: loss 1.780901551246643\n",
      "step 4000: loss 1.697569727897644\n",
      "step 5000: loss 1.755009412765503\n",
      "step 6000: loss 1.643255352973938\n",
      "step 7000: loss 1.556833267211914\n",
      "step 8000: loss 1.6296324729919434\n",
      "step 9000: loss 1.5706883668899536\n",
      "step 10000: loss 1.5679899454116821\n",
      "step 11000: loss 1.604675531387329\n",
      "step 12000: loss 1.5732502937316895\n",
      "step 13000: loss 1.5478116273880005\n",
      "step 14000: loss 1.518553614616394\n",
      "step 15000: loss 1.5375080108642578\n",
      "step 16000: loss 1.5100337266921997\n",
      "step 17000: loss 1.514094352722168\n",
      "step 18000: loss 1.428208589553833\n",
      "step 19000: loss 1.5041663646697998\n",
      "step 20000: loss 1.4818570613861084\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train loss (ema): 1.491172840211055\n",
      "final validation loss: 1.7365468740463257\n"
     ]
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "multi_block_model = MultiBlockModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8).to(device)\n",
    "losses, eval_losses = train(multi_block_model, 20000, batch_size, context_length)\n",
    "plot_ema(losses, eval_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model still overfits a bit here, but our output doesn't look half bad. It's still nonsense but sounds newsy-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trring has for people. He her asked polege, Internaties more than .,34,020 yearloas and Crisse. Builbams have last end medile-man export from by Aitter Csst Kaail Novembeers and Quobd this firster and towness of the Prans off Avabuas made 12,000 in NutchN. On Baterer 1, solders agreements intruges, uperson really exolue slinked the carlimment numbers tured: uknof haires politicy because if wantional abjoping in at nrim donate division. The Tans. They fushing from specive time his.  The ashives.ukn How is is a one of Edfield said donuknt is with a diard is treated. 'Then it's numberly (24,000 yantio. \"Nida EAWG:2 Zirda Trea, their one-reports was we whan leal get to their police years wanted Uirgin, for these of the deal Protector Chnourt Hething's will knees, Gladely in 2012 which her hear overtured by pressents have been healthn. Mondoye Bruem, old in the ditival to mobition will said whe three have been his also show. Then Kriter conclection.' Nugteremiusdary Mrstandance Pissign to sext Mon\n"
     ]
    }
   ],
   "source": [
    "generate_text(multi_block_model, None, 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to fine-tune the news model for shakespeare. It had a bit of a gap between train and validation loss but is still much larger than the shakespeare corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "spl = int(0.9 * len(data))\n",
    "\n",
    "train_data = data[:spl]\n",
    "\n",
    "val_data = data[spl:]\n",
    "\n",
    "def get_batch(split, batch_size, context_length):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning we need to choose which portions of the model we want to freeze, and which paramters we want to allow to be learned. I'll freeze the first 4 multi-head attention blocks by setting required_grad to be False, and allow the rest of the paramters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "finetuned_model = deepcopy(multi_block_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in finetuned_model.blocks[4:]:\n",
    "    for p in block.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: loss 1.4440546035766602\n",
      "step 2000: loss 1.4285008907318115\n",
      "step 3000: loss 1.5048296451568604\n",
      "step 4000: loss 1.4297479391098022\n",
      "step 5000: loss 1.5016945600509644\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train loss (ema): 1.4639546850904759\n",
      "final validation loss: 1.5518561601638794\n"
     ]
    }
   ],
   "source": [
    "losses, eval_losses = train(finetuned_model, 5000, batch_size, context_length, 1e-4)\n",
    "plot_ema(losses, eval_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seemed to work pretty well! We achieved a lower final validation loss than we did before -- and the train loss started at around 1.5, which is much lower than if we would have started from scratch. We were even able to improve further on the training loss by fine-tuning the non-frozen parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t three and do purncer you gentleman:\n",
      "Steal it be Aover obes? Peace this voice\n",
      "Har still had staughters and fly to weep to my\n",
      "To will bapegn? Has shore a nature to be dukele\n",
      "Their to-nife in traitls giils is one:\n",
      "We shall fellow once the bloodl of the poor's wash awas:\n",
      "to do fees, land and not nokens\n",
      "power crown the vice at ever affirm'd but dand.\n",
      "\n",
      "Apoloy;\n",
      "Wife, pless and might thou to strike thy trook\n",
      "Daties nother those too news, deing are war flame\n",
      "To anvoides me I say our long tone out,\n",
      "Madane he serve call and even as desperation,\n",
      "Shall this murder sinces absward.\n",
      "\n",
      "SABHON:\n",
      "For all, you warlikess!\n",
      "\n",
      "ROMEO:\n",
      "For you:\n",
      "Why sweett, he'll my uncle.\n",
      "\n",
      "CORIOLANUS:\n",
      "Take you rate-run.\n",
      "\n",
      "BRUTUS:\n",
      "Here judge, that have my home\n",
      "Thate wash of that sinly, with remert in the solding prize\n",
      "find kineven white; each occutaus brawded,\n",
      "And Oxperhall no the fearing as that adverct-out\n",
      "In thew hehore mine her her devom him:\n",
      "Mark me, no country, trains.\n",
      "\n",
      "SICINIUS:\n",
      "Ad move my lord brow eyes,\n",
      "That spreaden, have\n"
     ]
    }
   ],
   "source": [
    "generate_text(finetuned_model, None, 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: this indeed looks like shakespeare!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise I'll be implementing the LazyFormer architecture (https://arxiv.org/pdf/2102.12702.pdf)\n",
    "\n",
    "This architecture aims to reduce computation requirements by only calculating the key and query matrices once, and re-using them within a sequential block of *m* self-attention modules. \n",
    "\n",
    "To implement this, we can first implement a *LazyMultiHeadAttention* block, which is just the multi-head attention block we've implemented before minus the key and query dot product, we just pass that as a second argument called *softmax output*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length, num_heads, dropout=0.05) -> None:\n",
    "        assert output_channels % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.head_size = output_channels // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.value = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        idx, kq = inputs\n",
    "        B, T, C = idx.shape \n",
    "    \n",
    "        w = self.dropout(kq)\n",
    "        v = self.value(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        v = v.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "\n",
    "        # torch requires a reshape here for reasons I don't fully understand\n",
    "        attn_out = (w @ v).transpose(-3, -2).reshape(B, T, -1)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can implement the KQ dot-product as its own block, then pass this result through each of the lazy blocks by simply returning it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyKQ(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, context_length, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = output_channels // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "        self.key = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        self.query = nn.Linear(input_channels, output_channels, bias=False)\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        B, T, C = idx.shape \n",
    "        # B = batches\n",
    "        # T = timesteps\n",
    "        # C = input channels\n",
    "        # Nh = attention heads\n",
    "        # Hs = head size\n",
    "        \n",
    "        q = self.query(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        k = self.key(idx) # (B, T, C) -> (B, T, Nh * Hs)\n",
    "        \n",
    "        q = q.transpose(-2, -1).view(B, self.num_heads, self.head_size, T).transpose(-2, -1) # (B, T, Nh * Hs) -> (B, Nh, T, Hs)\n",
    "        k = k.transpose(-2, -1).view(B, self.num_heads, self.head_size, T) # (B, T, Nh * Hs) -> (B, Nh, Hs, T)\n",
    "        \n",
    "        w = q @ k # (B, Nh, T, Hs) @ (B, Nh, Hs, T) -> (B, Nh, T, T)\n",
    "        w = w.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "\n",
    "        w *= self.head_size**-0.5\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        \n",
    "        return w\n",
    "    \n",
    "class LazyFormerInnerBlock(nn.Module):\n",
    "    def __init__(self, emb_size, head_size, context_length, num_multi_attn_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attn = LazyMultiHeadAttention(emb_size, head_size, context_length, num_heads=num_multi_attn_heads)\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.norm1 = nn.LayerNorm(head_size)\n",
    "        self.norm2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, kq = inputs\n",
    "        x = self.attn((self.norm1(x), kq)) + x # residual\n",
    "        x = self.ff(self.norm2(x)) + x # residual\n",
    "        return (x, kq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bundles nicely together into a higher-level module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFormerBlock(nn.Module):\n",
    "    def __init__(self, m, input_channels, output_channels, context_length, num_heads):\n",
    "        super().__init__()\n",
    "        self.lazyblocks = nn.Sequential(\n",
    "            *[LazyFormerInnerBlock(input_channels, output_channels, context_length, num_heads) for _ in range(m)] \n",
    "        )\n",
    "        \n",
    "        self.lazy_kq = LazyKQ(input_channels, output_channels, context_length, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        (res, _) = self.lazyblocks((x, self.lazy_kq(x)))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class LazeFormerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, head_size, context_length, num_multi_attn_heads, num_blocks, m) -> None:\n",
    "        super().__init__()\n",
    "        assert head_size % num_multi_attn_heads == 0\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, emb_size)\n",
    "        self.positional_embeddings = nn.Embedding(context_length, emb_size)\n",
    "        self.blocks = nn.Sequential(*[LazyFormerBlock(m, emb_size, head_size, context_length, num_multi_attn_heads) for _ in range(num_blocks)], nn.LayerNorm(head_size))\n",
    "        self.ff = FeedForward(head_size, head_size)\n",
    "        self.output_layer = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # get token and positional embeddings\n",
    "        t = self.token_embeddings(idx) # (B, T) -> (B, T, C)\n",
    "        p = self.positional_embeddings(torch.arange(T, device=idx.device)) # (T, C)\n",
    "        x = t + p # (B, T, C) (broadcasting)\n",
    "\n",
    "        # pass thru attention head\n",
    "        x = self.blocks(x) # (B, T, C) -> (B, T, H)\n",
    "        \n",
    "        # pass thru feed forward\n",
    "        x = self.ff(x) # (B, T, H) -> (B, T, H)\n",
    "\n",
    "        # pass thru output layer\n",
    "        logits = self.output_layer(x) # (B, T, H) -> (B, T, V)\n",
    "\n",
    "        \n",
    "        if targets is not None:\n",
    "            B, T, V = logits.shape\n",
    "            logits = logits.view(B*T, V)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_crop = idx[:,-self.context_length:]\n",
    "            logits, _ = self(idx_crop)\n",
    "            logits = logits[:,-1,:] # all batches, last timestamp, all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compute savings we get decreases our training time, so we can devote more resources towards scaling up our model if we choose to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: loss 2.222447633743286\n",
      "step 1000: loss 1.9997532367706299\n",
      "step 1500: loss 1.8058192729949951\n",
      "step 2000: loss 1.7145823240280151\n",
      "step 2500: loss 1.6224730014801025\n",
      "step 3000: loss 1.5572205781936646\n",
      "step 3500: loss 1.5780043601989746\n",
      "step 4000: loss 1.565028190612793\n",
      "step 4500: loss 1.4497921466827393\n",
      "step 5000: loss 1.4408061504364014\n",
      "step 5500: loss 1.513249158859253\n",
      "step 6000: loss 1.4738184213638306\n",
      "step 6500: loss 1.4893865585327148\n",
      "step 7000: loss 1.4597078561782837\n",
      "step 7500: loss 1.4414904117584229\n",
      "step 8000: loss 1.401940107345581\n",
      "step 8500: loss 1.4191604852676392\n",
      "step 9000: loss 1.4558546543121338\n",
      "step 9500: loss 1.3784352540969849\n",
      "step 10000: loss 1.379726767539978\n",
      "step 10500: loss 1.4191715717315674\n",
      "step 11000: loss 1.3507015705108643\n",
      "step 11500: loss 1.3854241371154785\n",
      "step 12000: loss 1.390730619430542\n",
      "step 12500: loss 1.34495210647583\n",
      "step 13000: loss 1.405100703239441\n",
      "step 13500: loss 1.378846287727356\n",
      "step 14000: loss 1.33842134475708\n",
      "step 14500: loss 1.4109824895858765\n",
      "step 15000: loss 1.3368926048278809\n",
      "step 15500: loss 1.3202470541000366\n",
      "step 16000: loss 1.4135640859603882\n",
      "step 16500: loss 1.3262356519699097\n",
      "step 17000: loss 1.2979457378387451\n",
      "step 17500: loss 1.295981526374817\n",
      "step 18000: loss 1.3773459196090698\n",
      "step 18500: loss 1.409471869468689\n",
      "step 19000: loss 1.3397924900054932\n",
      "step 19500: loss 1.4009541273117065\n",
      "step 20000: loss 1.3158777952194214\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train loss (ema): 1.329646972399085\n",
      "final validation loss: 1.5169371366500854\n"
     ]
    }
   ],
   "source": [
    "context_length = 128\n",
    "batch_size = 32\n",
    "emb_size = 64\n",
    "m = 4\n",
    "multi_block_model = LazeFormerModel(vocab_size, emb_size, emb_size, context_length, num_multi_attn_heads=8, num_blocks=8, m=m).to(device)\n",
    "losses, eval_losses = train(multi_block_model, 20000, batch_size, context_length, print_every=500)\n",
    "plot_ema(losses, eval_losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying this technique we were able to acheive an even lower validation loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tly wearms upon, as if make the sdeep\n",
      "And that she lives his person; by the norther lies.\n",
      "\n",
      "EXETER:\n",
      "Where I did not should would busing, better\n",
      "Rogeo?\n",
      "\n",
      "HERMIONE:\n",
      "Not a voices\n",
      "With go the sovereign, away, leaves his sir;\n",
      "Do more fault Aptaticion: then he did.\n",
      "It shall be a skarle Edward in thee.\n",
      "\n",
      "MARCIUS:\n",
      "Should I did not: I have lief to lay is;\n",
      "So swain'd, as seen the rescront?\n",
      "\n",
      "Shepherd:\n",
      "But, gentle poor backs, if thou wert you,\n",
      "Which many or sweetn, for, my face, to see foul\n",
      "the queen his time vized passion\n",
      "think you do not go vant of Armiliable.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "'Tis bacrike me but give me sphoming rough.\n",
      "\n",
      "ROMEO:\n",
      "Nay, night, and grat's thought.\n",
      "\n",
      "MAMILLIUS:\n",
      "Wilt not vame offends, Claudions of alred throng\n",
      "of deservide.\n",
      "\n",
      "WARWICK:\n",
      "Come, cousin, I have rine\n",
      "If you.\n",
      "\n",
      "CORIOLANUS:\n",
      "Which she is not\n",
      "mother, after there, that men as officer.\n",
      "Come, good crown, Marsa, or the gods,\n",
      "As this king; and then most teapor plustity.\n",
      "\n",
      "MENENIUS:\n",
      "O, so rave hokses!\n",
      "And no sovereign and which ever,\n",
      "My sid\n"
     ]
    }
   ],
   "source": [
    "generate_text(multi_block_model, None, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
